\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{kulesza2012learning}
\@input{nomenclature.aux}
\citation{mehta1960density}
\citation{benard1973detection}
\citation{johansson2004determinantal}
\citation{borodin2010adding}
\citation{tao2010universality}
\citation{bourgade2013quantum}
\citation{borodin2009determinantal}
\citation{hough2006determinantal}
\citation{lyons2003determinantal}
\citation{kulesza2011k}
\citation{kulesza2012learning1}
\@writefile{toc}{\contentsline {chapter}{Introduction}{1}{chapter*.1}}
\citation{kulesza2012learning}
\citation{urschel2017learning}
\@writefile{toc}{\contentsline {subsubsection}{Outline of the thesis}{2}{subsubsection*.2}}
\@writefile{toc}{\contentsline {subsubsection}{Contributions}{2}{subsubsection*.3}}
\citation{kulesza2012determinantal}
\@writefile{toc}{\contentsline {chapter}{\numberline {\textup  {I}}Determinantal point processes: Basic notions and properties}{3}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {\textup  {I}.1}Definitions and properties}{3}{section.1.1}}
\citation{borodin2009determinantal}
\newlabel{e2.1}{{1.1}{4}{Determinantal point process}{equation.1.1.1}{}}
\newlabel{e2.2}{{1.2}{4}{Repulsive behaviour of DPPs}{equation.1.1.2}{}}
\citation{kulesza2012learning}
\@writefile{lof}{\contentsline {figure}{\numberline {\textup  {I}.1}{\ignorespaces A DPP with negative correlations of close points on a \(100\times 100\) grid in the unit square on the left and a Poisson point process on the same grid on the right with the same expected cardinality. The -- in this case spatially -- repellent structure of the DPP is clearly visible.}}{5}{figure.1.1}}
\newlabel{fig:1}{{\textup  {I}.1}{5}{A DPP with negative correlations of close points on a \(100\times 100\) grid in the unit square on the left and a Poisson point process on the same grid on the right with the same expected cardinality. The -- in this case spatially -- repellent structure of the DPP is clearly visible}{figure.1.1}{}}
\newlabel{e2.2.1}{{1.3}{5}{\(L\)-ensembles}{equation.1.1.3}{}}
\newlabel{e2.3}{{1.4}{5}{\(L\)-ensembles}{equation.1.1.4}{}}
\newlabel{e2.4}{{1.5}{6}{}{equation.1.1.5}{}}
\citation{kulesza2012determinantal}
\citation{kulesza2012determinantal}
\@writefile{lof}{\contentsline {figure}{\numberline {\textup  {I}.2}{\ignorespaces The first line (a) illustrates the volumes spanned by vectors, and in the second line it can be seen how this volume increases if the length -- associated with the quality -- increases (b) and decreases if they become more similar in direction which we interpret as two items becoming more similar (c). Original graphic from \cite  {kulesza2012determinantal}.}}{7}{figure.1.2}}
\newlabel{fig:2}{{\textup  {I}.2}{7}{The first line (a) illustrates the volumes spanned by vectors, and in the second line it can be seen how this volume increases if the length -- associated with the quality -- increases (b) and decreases if they become more similar in direction which we interpret as two items becoming more similar (c). Original graphic from \cite {kulesza2012determinantal}}{figure.1.2}{}}
\newlabel{moddiv}{{1.7}{7}{Modelling diversity over distance}{satz.1.7}{}}
\citation{leeindividualness}
\citation{kulesza2010structured}
\citation{bardenet2015inference}
\citation{emiris2005improved}
\citation{kulesza2012determinantal}
\citation{kulesza2012determinantal}
\citation{djolonga2014map}
\citation{gillenwater2012near}
\citation{kulesza2011k}
\@writefile{toc}{\contentsline {section}{\numberline {\textup  {I}.2}Variations of DPPs}{10}{section.1.2}}
\citation{kulesza2010structured}
\citation{rezakhanlou2012lectures}
\@writefile{toc}{\contentsline {section}{\numberline {\textup  {I}.3}Simulation and Existence of DPPs}{12}{section.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\textup  {I}.3.1}Cauchy-Binet type identities}{12}{subsection.1.3.1}}
\newlabel{CB-type}{{1.16}{14}{Variation of Cauchy-Binet}{satz.1.16}{}}
\citation{hough2006determinantal}
\citation{kulesza2012determinantal}
\newlabel{CBP}{{1.7}{15}{Cauchy-Binet type identities}{equation.1.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\textup  {I}.3.2}Sampling and Existence}{15}{subsection.1.3.2}}
\newlabel{mixDPPs}{{1.17}{15}{Mixture representation of DPPs}{satz.1.17}{}}
\newlabel{BerKer}{{1.8}{15}{Mixture representation of DPPs}{equation.1.3.8}{}}
\newlabel{eeDPPs}{{1.19}{15}{Existence of elementary DPPs}{satz.1.19}{}}
\newlabel{elemDPPs}{{1.9}{15}{Existence of elementary DPPs}{equation.1.3.9}{}}
\newlabel{alg:elementary-DPP-sampling}{{1}{16}{Sampling and Existence}{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Sampling from an elementary DPP }}{16}{algorithm.1}}
\citation{kulesza2012determinantal}
\newlabel{alg:DPP-sampling}{{2}{18}{Sampling and Existence}{algorithm.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Sampling from a DPP }}{18}{algorithm.2}}
\newlabel{carDPP}{{1.23}{18}{Cardinality of DPPs}{satz.1.23}{}}
\@writefile{toc}{\contentsline {subsubsection}{Possible improvements}{19}{subsubsection*.4}}
\citation{magen2008near}
\citation{kulesza2012learning}
\@writefile{toc}{\contentsline {section}{\numberline {\textup  {I}.4}Simulation of toy examples}{20}{section.1.4}}
\@writefile{toc}{\contentsline {subsubsection}{Points on a line}{20}{subsubsection*.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {\textup  {I}.3}{\ignorespaces Two DPPs with a different strength of repulsion on the left and a Poisson point process on a (discretised) line with same expected cardinality. The spatial repulsion of the DPPs is clearly visible.}}{21}{figure.1.3}}
\newlabel{pointsonaline}{{\textup  {I}.3}{21}{Two DPPs with a different strength of repulsion on the left and a Poisson point process on a (discretised) line with same expected cardinality. The spatial repulsion of the DPPs is clearly visible}{figure.1.3}{}}
\citation{ruschendorf2014mathematische}
\citation{borodin2010adding}
\@writefile{toc}{\contentsline {subsubsection}{Points in a square}{23}{subsubsection*.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {\textup  {I}.4}{\ignorespaces A DPP (right) and a Poisson point process (left) on a \(100\times 100\) grid in the unit square with the same expected cardinality. The -- in this case spatially -- repellent structure of the DPP is clearly visible.}}{23}{figure.1.4}}
\newlabel{pointsinthesquare}{{\textup  {I}.4}{23}{A DPP (right) and a Poisson point process (left) on a \(100\times 100\) grid in the unit square with the same expected cardinality. The -- in this case spatially -- repellent structure of the DPP is clearly visible}{figure.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\textup  {I}.5}{\ignorespaces A DPP on a \(100\times 100\) grid in the unit square with decreasing quality towards the edges.}}{24}{figure.1.5}}
\newlabel{pointsinthesquarelog}{{\textup  {I}.5}{24}{A DPP on a \(100\times 100\) grid in the unit square with decreasing quality towards the edges}{figure.1.5}{}}
\citation{newton1744philosophiae}
\@writefile{toc}{\contentsline {chapter}{\numberline {\textup  {II}}Point estimators and parametric models}{25}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {\textup  {II}.1}Kernel reconstruction from the empirical measures}{26}{section.2.1}}
\citation{urschel2017learning}
\citation{rising2015efficient}
\newlabel{thrtimthr}{{2.1}{28}{Kernel reconstruction from the empirical measures}{equation.2.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\textup  {II}.1.1}Graph theoretical concepts}{28}{subsection.2.1.1}}
\citation{10.2307/1967604}
\citation{bondy2011graph}
\newlabel{examplegraphs}{{\textup  {II}.1.1}{30}{Graph theoretical concepts}{Item.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\textup  {II}.1}{\ignorespaces Some examples of graphs and cycles. The first sketch shows a graph and the three other ones subsgraphs of it where the edges not belonging to the subgraph are depicted dashed. The first one is a symple chordless cycle, the second one a simple but not chordless cycle and the last one is not a cycle at all.}}{30}{figure.2.1}}
\newlabel{makesimple}{{\textup  {II}.1.1}{30}{Graph theoretical concepts}{satz.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\textup  {II}.2}{\ignorespaces Illustration of the search for a simple cycle in a graph with degrees greater than two. Once a maximal non intersecting path like \(12543\) is selected, every continuation of the path -- in this case 2 or 1 -- is already present in the path and therefore induces a simple cycle.}}{30}{figure.2.2}}
\citation{kulesza2012learning}
\newlabel{makechordless}{{\textup  {II}.1.1}{31}{Graph theoretical concepts}{figure.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\textup  {II}.3}{\ignorespaces The simple cycle \(123451\) on the left is not chordless but the symmetric difference of the two simple chordless cycles \(1231\) and \(13451\) on the right.}}{31}{figure.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\textup  {II}.1.2}The solution of the principal minor assignment problem}{31}{subsection.2.1.2}}
\newlabel{e3.0}{{\textup  {II}.1.2}{31}{The solution of the principal minor assignment problem}{satz.2.6}{}}
\newlabel{pmcl}{{2.2}{32}{Principal minors of simple chordless cycles}{equation.2.1.2}{}}
\newlabel{leib}{{2.3}{32}{The solution of the principal minor assignment problem}{equation.2.1.3}{}}
\newlabel{pm}{{\textup  {II}.1.2}{33}{The solution of the principal minor assignment problem}{equation.2.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\textup  {II}.4}{\ignorespaces An easy example for the two kinds of permutations of a chordless simple cycle that maps vertices to neighbors.}}{33}{figure.2.4}}
\newlabel{e3.2}{{2.4}{33}{The solution of the principal minor assignment problem}{equation.2.1.4}{}}
\newlabel{e3.3}{{2.5}{33}{The solution of the principal minor assignment problem}{equation.2.1.5}{}}
\newlabel{cec}{{2.12}{34}{Construction of the equivalence class}{satz.2.12}{}}
\citation{urschel2017learning}
\citation{rising2015efficient}
\citation{urschel2017learning}
\newlabel{linEqu}{{2.6}{35}{Construction of the equivalence class}{equation.2.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\textup  {II}.1.3}Definition of the estimator and consistency}{35}{subsection.2.1.3}}
\newlabel{ass}{{2.13}{35}{Assumption}{satz.2.13}{}}
\newlabel{DefEst}{{2.14}{35}{Definition of the estimator}{satz.2.14}{}}
\newlabel{LinEqu2}{{2.7}{36}{Definition of the estimator}{equation.2.1.7}{}}
\newlabel{MomCon}{{2.16}{36}{Consistency}{satz.2.16}{}}
\citation{urschel2017learning}
\citation{horton1987polynomial}
\citation{amaldi2010efficient}
\newlabel{asc}{{2.8}{37}{Definition of the estimator and consistency}{equation.2.1.8}{}}
\newlabel{ConProb1}{{2.9}{37}{Definition of the estimator and consistency}{equation.2.1.9}{}}
\newlabel{ConProb2}{{2.10}{37}{Definition of the estimator and consistency}{equation.2.1.10}{}}
\citation{rice2006mathematical}
\@writefile{toc}{\contentsline {section}{\numberline {\textup  {II}.2}Maximum likelihood estimation}{38}{section.2.2}}
\citation{boyd2004convex}
\newlabel{mledef}{{2.11}{39}{Maximum likelihood estimator}{equation.2.2.11}{}}
\newlabel{dirmet}{{2.20}{40}{Existence of maximisers}{satz.2.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\textup  {II}.2.1}Presentation of different models}{40}{subsection.2.2.1}}
\newlabel{models}{{\textup  {II}.2.1}{40}{Presentation of different models}{subsection.2.2.1}{}}
\newlabel{e3.1}{{2.12}{41}{}{equation.2.2.12}{}}
\newlabel{loglikqua}{{2.13}{42}{}{equation.2.2.13}{}}
\newlabel{e3.31}{{2.14}{43}{}{equation.2.2.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\textup  {II}.2.2}Coercivity and existence of the maximum likelihood estimators}{43}{subsection.2.2.2}}
\newlabel{PosMLE}{{2.27}{45}{Positivity of the MLE}{satz.2.27}{}}
\citation{newey1994large}
\@writefile{toc}{\contentsline {subsection}{\numberline {\textup  {II}.2.3}Consistency of the maximum likelihood estimators}{48}{subsection.2.2.3}}
\newlabel{entropy}{{2.15}{48}{Formal proof of consistency}{equation.2.2.15}{}}
\newlabel{conext}{{2.35}{49}{Consistency of extremal estimators}{satz.2.35}{}}
\newlabel{con2}{{2.16}{49}{Consistency of extremal estimators}{equation.2.2.16}{}}
\newlabel{con1}{{2.17}{49}{Consistency of extremal estimators}{equation.2.2.17}{}}
\citation{martin2011mathematical}
\citation{mackay2003information}
\citation{volkenstein2009entropy}
\citation{gray1990entropy}
\citation{newey1994large}
\citation{lehmann2006theory}
\citation{tauchen1985diagnostic}
\newlabel{infine}{{2.38}{51}{Information inequality}{satz.2.38}{}}
\newlabel{entropybound}{{2.18}{51}{}{equation.2.2.18}{}}
\newlabel{locunicon}{{2.39}{52}{Locally uniform convergence}{satz.2.39}{}}
\newlabel{concom}{{2.40}{53}{Control outside of a compact set}{satz.2.40}{}}
\newlabel{ent}{{2.19}{55}{}{equation.2.2.19}{}}
\newlabel{elepro}{{2.20}{55}{}{equation.2.2.20}{}}
\citation{vavasis1995complexity}
\citation{kulesza2012learning}
\citation{mariet2015fixed}
\@writefile{toc}{\contentsline {subsection}{\numberline {\textup  {II}.2.4}Approximation of the MLE}{56}{subsection.2.2.4}}
\newlabel{loglikagain}{{2.21}{56}{}{equation.2.2.21}{}}
\newlabel{logliklogmodagain}{{2.22}{57}{}{equation.2.2.22}{}}
\citation{boyd2004convex}
\citation{kulesza2012learning1}
\@writefile{toc}{\contentsline {subsection}{\numberline {\textup  {II}.2.5}Further learning approaches}{58}{subsection.2.2.5}}
\citation{kulesza2011k}
\@writefile{toc}{\contentsline {subsubsection}{Learning for conditional DPPs}{59}{subsubsection*.7}}
\@writefile{toc}{\contentsline {subsubsection}{Estimating the mixture coefficients of \(k\)-DPPs}{59}{subsubsection*.8}}
\@writefile{toc}{\contentsline {subsubsection}{Learning the repulsiveness of a DPP}{60}{subsubsection*.9}}
\citation{affandi2014learning}
\citation{rice2006mathematical}
\@writefile{toc}{\contentsline {chapter}{\numberline {\textup  {III}}Bayesian learning for DPPs}{61}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {\textup  {III}.1}Bayesian approach to parameter estimation}{61}{section.3.1}}
\newlabel{post}{{3.1}{62}{Bayesian approach to parameter estimation}{equation.3.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\textup  {III}.1}{\ignorespaces Approximated posterior density of the two dimensional log linearity constant of a two dimensional DPP with a uniform distribution as a prior. The MLE estimator is marked green and is at the mode of the distribution.}}{63}{figure.3.1}}
\newlabel{fig:4.1}{{\textup  {III}.1}{63}{Approximated posterior density of the two dimensional log linearity constant of a two dimensional DPP with a uniform distribution as a prior. The MLE estimator is marked green and is at the mode of the distribution}{figure.3.1}{}}
\newlabel{genpost}{{3.2}{64}{Bayesian approach without prior}{equation.3.1.2}{}}
\newlabel{post2}{{3.3}{64}{}{equation.3.1.3}{}}
\citation{kass1995bayes}
\citation{kass1995bayes}
\citation{meyn2012markov}
\citation{robert2013monte}
\newlabel{norma}{{3.4}{65}{}{equation.3.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Model selection using the Bayes factor}{65}{subsubsection*.10}}
\@writefile{lot}{\contentsline {table}{\numberline {\textup  {III}.1}{\ignorespaces Interpretation of how strongly different values of \(K\) imply that the first model is a better description of the data than the second one.}}{66}{table.3.1}}
\newlabel{tab:BayesFactor}{{\textup  {III}.1}{66}{Interpretation of how strongly different values of \(K\) imply that the first model is a better description of the data than the second one}{table.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {\textup  {III}.2}Markov chain Monte Carlo methods}{66}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\textup  {III}.2.1}Reminder on Markov chains}{66}{subsection.3.2.1}}
\citation{le2016brownian}
\newlabel{MC}{{3.5}{67}{Markov chain}{equation.3.2.5}{}}
\citation{meyn2012markov}
\citation{dudley2010distances}
\newlabel{ergTheo}{{3.13}{68}{Ergodic theorem}{satz.3.13}{}}
\newlabel{ergo}{{3.6}{68}{Ergodic theorem}{equation.3.2.6}{}}
\citation{metropolis1953equation}
\citation{robert2013monte}
\@writefile{toc}{\contentsline {subsection}{\numberline {\textup  {III}.2.2}Metropolis-Hastings random walk}{69}{subsection.3.2.2}}
\newlabel{threshold}{{3.7}{70}{The MH random walk}{equation.3.2.7}{}}
\newlabel{alg:MH}{{3}{70}{The MH random walk}{algorithm.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces A single step of the MH random walk }}{70}{algorithm.3}}
\newlabel{calc1}{{3.8}{71}{Metropolis-Hastings random walk}{equation.3.2.8}{}}
\citation{robert2013monte}
\citation{robert1999metropolis}
\citation{roberts1997weak}
\citation{robert1999metropolis}
\newlabel{example}{{3.22}{73}{One dimensional MH}{satz.3.22}{}}
\citation{roberts2009examples}
\newlabel{tuning}{{3.23}{74}{Tuning the proposal}{satz.3.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\textup  {III}.2.3}Slice sampling}{74}{subsection.3.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {\textup  {III}.2}{\ignorespaces Histograms and autocorrelation functions of for three different variances \(\alpha \) of the Gaussian proposal distributions. It is apparent that the histogram for \(\alpha =3\) fits the actual density the best and also that the autocorrelation decays the quickest for this parameter. Note that for \(\alpha =0.01\) the MH random walk only explored some area of high density. The actual density if obtained by numerical integration.}}{75}{figure.3.2}}
\newlabel{fig:4.1.2}{{\textup  {III}.2}{75}{Histograms and autocorrelation functions of for three different variances \(\alpha \) of the Gaussian proposal distributions. It is apparent that the histogram for \(\alpha =3\) fits the actual density the best and also that the autocorrelation decays the quickest for this parameter. Note that for \(\alpha =0.01\) the MH random walk only explored some area of high density. The actual density if obtained by numerical integration}{figure.3.2}{}}
\citation{neal2003slice}
\citation{neal2003slice}
\@writefile{lof}{\contentsline {figure}{\numberline {\textup  {III}.3}{\ignorespaces Schematic sketch of the selection of a slice: (a) first \(y\) is sampled uniformly in \([0, f(x_0)]\) and (b) the slice is selected. Original graphic from \cite  {neal2003slice}.}}{76}{figure.3.3}}
\newlabel{fig:4.2}{{\textup  {III}.3}{76}{Schematic sketch of the selection of a slice: (a) first \(y\) is sampled uniformly in \([0, f(x_0)]\) and (b) the slice is selected. Original graphic from \cite {neal2003slice}}{figure.3.3}{}}
\newlabel{alg:slice-sampling}{{4}{77}{The slice sampling method}{algorithm.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces A single slice sampling step }}{77}{algorithm.4}}
\citation{mira2002efficiency}
\newlabel{alg:slice-sample}{{5}{79}{}{algorithm.5}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Sampling from a uniform distribution on a subset \(S\subseteq C\) }}{79}{algorithm.5}}
\newlabel{alg:cuboid}{{6}{79}{}{algorithm.6}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Sampling a random cuboid }}{79}{algorithm.6}}
\citation{neal2003slice}
\newlabel{alg:slice-sampling-implementation}{{7}{80}{}{algorithm.7}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces Algorithm for the slice sampling }}{80}{algorithm.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\textup  {III}.2.4}Variational MCMC methods}{81}{subsection.3.2.4}}
\newlabel{post3}{{3.9}{81}{Variational MCMC methods}{equation.3.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\textup  {III}.4}{\ignorespaces Histograms and autocorrelation functions for the choices of \(\alpha = 0.01, 0.5, 10\). The auto correlation obviously decreases the fastest for \(\alpha =0.01\), however the computation time is much higher than for the parameter the other parameters.}}{82}{figure.3.4}}
\newlabel{fig:4.3}{{\textup  {III}.4}{82}{Histograms and autocorrelation functions for the choices of \(\alpha = 0.01, 0.5, 10\). The auto correlation obviously decreases the fastest for \(\alpha =0.01\), however the computation time is much higher than for the parameter the other parameters}{figure.3.4}{}}
\citation{affandi2014learning}
\citation{bardenet2015inference}
\newlabel{alg:variational-MH}{{8}{84}{}{algorithm.8}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {8}{\ignorespaces One step in the variational MH random walk }}{84}{algorithm.8}}
\newlabel{alg:decide}{{9}{84}{}{algorithm.9}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {9}{\ignorespaces Deciding \(f(x)\ge y\) through the bounds }}{84}{algorithm.9}}
\@writefile{toc}{\contentsline {chapter}{\numberline {\textup  {IV}}Toy example: Learning the log linearity constant of a spatial DPP}{85}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {\textup  {IV}.1}MLE and regularised MLE}{86}{section.4.1}}
\newlabel{mletheta}{{4.1}{86}{MLE and regularised MLE}{equation.4.1.1}{}}
\newlabel{toyprior}{{4.2}{86}{MLE and regularised MLE}{equation.4.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {\textup  {IV}.2}Bayesian estimation using MCMC methods}{87}{section.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {\textup  {IV}.1}{\ignorespaces A plot of the first burn in period with two different starting points -- the first one being the origin, the second one \((10, -10)\). The regularised MLE for the log linearity constant is marked by the green cross and the mean or centre of mass of the second half of the random walk by the red cross.}}{88}{figure.4.1}}
\newlabel{fig:findingstart}{{\textup  {IV}.1}{88}{A plot of the first burn in period with two different starting points -- the first one being the origin, the second one \((10, -10)\). The regularised MLE for the log linearity constant is marked by the green cross and the mean or centre of mass of the second half of the random walk by the red cross}{figure.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\textup  {IV}.2}{\ignorespaces Plots of the two state parameters of the MH random walk starting at the origin. The acceptance rate drops significantly and hardly any proposals are accepted.}}{88}{figure.4.2}}
\newlabel{fig:MCplot}{{\textup  {IV}.2}{88}{Plots of the two state parameters of the MH random walk starting at the origin. The acceptance rate drops significantly and hardly any proposals are accepted}{figure.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\textup  {IV}.3}{\ignorespaces A plot of the samples of the MH of the second burn in period. One can see how the points are distributed around the MLE which is marked green. Their empirical covariance will be used to tune the proposal.}}{89}{figure.4.3}}
\newlabel{fig:tuning}{{\textup  {IV}.3}{89}{A plot of the samples of the MH of the second burn in period. One can see how the points are distributed around the MLE which is marked green. Their empirical covariance will be used to tune the proposal}{figure.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\textup  {IV}.4}{\ignorespaces State plots of the second burn in period on the top and the final MH random walk on the bottom. One can see how the tuned proposal gives a higher acceptance rate in the final simulation.}}{90}{figure.4.4}}
\newlabel{fig:mixim-tuning}{{\textup  {IV}.4}{90}{State plots of the second burn in period on the top and the final MH random walk on the bottom. One can see how the tuned proposal gives a higher acceptance rate in the final simulation}{figure.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\textup  {IV}.5}{\ignorespaces A plot of the autocorrelation functions of the second burn in period and the final MH random walk. The latter one decreases faster which hints to a faster convergence due to the tuned proposal distributions.}}{91}{figure.4.5}}
\newlabel{fig:acf-sbi-final}{{\textup  {IV}.5}{91}{A plot of the autocorrelation functions of the second burn in period and the final MH random walk. The latter one decreases faster which hints to a faster convergence due to the tuned proposal distributions}{figure.4.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\textup  {IV}.6}{\ignorespaces Heat map of the MH random walks with \(10^4\) iterations. The regularised MLE estimator is shown as a white, the MLE as a green and the actual parameter as a red cross. The regularised MLE is the maximum of the (approximated) posterior.}}{91}{figure.4.6}}
\newlabel{fig:MHrw}{{\textup  {IV}.6}{91}{Heat map of the MH random walks with \(10^4\) iterations. The regularised MLE estimator is shown as a white, the MLE as a green and the actual parameter as a red cross. The regularised MLE is the maximum of the (approximated) posterior}{figure.4.6}{}}
\newlabel{ergTheo2}{{4.5}{92}{Ergodic theorem}{satz.4.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\textup  {IV}.7}{\ignorespaces Heat map of the MH random walks with \(10^4\) iterations. The regularised MLE estimator is shown as a white, the MLE as a green and the actual parameter as a red cross. The regularised MLE is the maximum of the (approximated) posterior.}}{93}{figure.4.7}}
\newlabel{fig:MHrwwup}{{\textup  {IV}.7}{93}{Heat map of the MH random walks with \(10^4\) iterations. The regularised MLE estimator is shown as a white, the MLE as a green and the actual parameter as a red cross. The regularised MLE is the maximum of the (approximated) posterior}{figure.4.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\textup  {IV}.8}{\ignorespaces Approximations of the posterior (on the left) and of the likelihood function (on the right) obtained by the interpolation between breakpoints. Just like in the approximations using MCMC methods, the reguralised MLE is marked white, the MLE green and the actual parameter red.}}{94}{figure.4.8}}
\newlabel{fig:directHeatMaps}{{\textup  {IV}.8}{94}{Approximations of the posterior (on the left) and of the likelihood function (on the right) obtained by the interpolation between breakpoints. Just like in the approximations using MCMC methods, the reguralised MLE is marked white, the MLE green and the actual parameter red}{figure.4.8}{}}
\@writefile{tdo}{\contentsline {todo}{find a reference}{94}{section*.11}}
\pgfsyspdfmark {pgfid5}{34459240}{18945595}
\pgfsyspdfmark {pgfid6}{3253820}{18945954}
\pgfsyspdfmark {pgfid7}{6174307}{18703399}
\@writefile{tdo}{\contentsline {todo}{is this true?}{94}{section*.12}}
\pgfsyspdfmark {pgfid10}{9651389}{16628237}
\pgfsyspdfmark {pgfid11}{3253820}{16628596}
\pgfsyspdfmark {pgfid12}{6174307}{16386041}
\newlabel{complexityMCMC}{{4.3}{95}{}{equation.4.2.3}{}}
\newlabel{complexityDirect}{{4.4}{95}{}{equation.4.2.4}{}}
\@writefile{tdo}{\contentsline {todo}{How does \(T\) grow with \(M\)?}{95}{section*.13}}
\pgfsyspdfmark {pgfid15}{13149706}{11794103}
\@writefile{tdo}{\contentsline {todo}{comment on complexity for silce sampling?}{95}{section*.14}}
\pgfsyspdfmark {pgfid20}{13149706}{11794103}
\pgfsyspdfmark {pgfid18}{35871692}{11794462}
\pgfsyspdfmark {pgfid19}{38792179}{11551907}
\pgfsyspdfmark {pgfid23}{35871692}{9043703}
\pgfsyspdfmark {pgfid24}{38792179}{8801148}
\citation{buhlmann2011statistics}
\@writefile{toc}{\contentsline {section}{\numberline {\textup  {IV}.3}Stability under noise -- does the regularisation help?}{96}{section.4.3}}
\@writefile{toc}{\contentsline {subsubsection}{Experiments}{97}{subsubsection*.15}}
\@writefile{lot}{\contentsline {table}{\numberline {\textup  {IV}.1}{\ignorespaces Table with the MLE and regularised MLE for noisy data perturbed by a Poisson point process with intensity \(\rho = \frac  1{400}\).}}{98}{table.4.1}}
\newlabel{tab:MLEvsRegMLE}{{\textup  {IV}.1}{98}{Table with the MLE and regularised MLE for noisy data perturbed by a Poisson point process with intensity \(\rho = \frac 1{400}\)}{table.4.1}{}}
\@writefile{tdo}{\contentsline {todo}{make this more scientific}{98}{section*.16}}
\pgfsyspdfmark {pgfid25}{12374744}{21347392}
\pgfsyspdfmark {pgfid26}{3253820}{21347751}
\pgfsyspdfmark {pgfid27}{6174307}{21105196}
\citation{urschel2017learning}
\@writefile{toc}{\contentsline {chapter}{\numberline {\textup  {V}}Summary and conclusion}{99}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsubsection}{Further work}{100}{subsubsection*.17}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Auxiliary results}{101}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Generated code}{102}{appendix.B}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Sampling algorithm}{102}{section.B.1}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Implementation of the MCMC methods and toy examples}{103}{section.B.2}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}MLE and Bayesian estimation of the log linearity constant}{105}{section.B.3}}
\@writefile{toc}{\contentsline {chapter}{Nomenclature}{108}{chapter*.18}}
\citation{*}
\bibstyle{alpha}
\bibdata{thesis}
\bibcite{affandi2014learning}{AFAT14}
\bibcite{amaldi2010efficient}{AIR10}
\bibcite{bardenet2015inference}{BA15}
\bibcite{borodin2010adding}{BDF10}
\bibcite{billingsley2013convergence}{Bil13}
\bibcite{bourgade2013quantum}{BK13}
\bibcite{benard1973detection}{BM73}
\bibcite{bondy2011graph}{BM11}
\bibcite{borodin2009determinantal}{Bor09}
\bibcite{boyd2004convex}{BV04}
\bibcite{buhlmann2011statistics}{BVDG11}
\bibcite{djolonga2014map}{DK14}
\bibcite{dudley2010distances}{Dud10}
\bibcite{emiris2005improved}{EP05}
\bibcite{gillenwater2012near}{GKT12}
\bibcite{gray1990entropy}{Gra90}
\bibcite{griffin2006principal}{GT06}
\bibcite{hastings1970monte}{Has70}
\bibcite{higham1990exploiting}{Hig90}
\bibcite{hough2006determinantal}{HKP{$^{+}$}06}
\bibcite{horton1987polynomial}{Hor87}
\bibcite{johansson2004determinantal}{Joh04}
\bibcite{kass1995bayes}{KR95}
\bibcite{kulesza2010structured}{KT10}
\bibcite{kulesza2011k}{KT11}
\bibcite{kulesza2012learning1}{KT12a}
\bibcite{kulesza2012determinantal}{KT{$^{+}$}12b}
\bibcite{kulesza2012learning}{Kul12}
\bibcite{lehmann2006theory}{LC06}
\bibcite{leeindividualness}{LCYO}
\bibcite{le2016brownian}{LG{$^{+}$}16}
\bibcite{lyons2003determinantal}{Lyo03}
\bibcite{mackay2003information}{Mac03}
\bibcite{martin2011mathematical}{ME11}
\bibcite{mehta1960density}{MG60}
\bibcite{metropolis1953equation}{MRR{$^{+}$}53}
\bibcite{mariet2015fixed}{MS15}
\bibcite{mira2002efficiency}{MT02}
\bibcite{meyn2012markov}{MT12}
\bibcite{magen2008near}{MZ08}
\bibcite{neal2003slice}{Nea03}
\bibcite{newton1744philosophiae}{NH44}
\bibcite{newey1994large}{NM94}
\bibcite{robert2013monte}{RC13}
\bibcite{rezakhanlou2012lectures}{Rez12}
\bibcite{roberts1997weak}{RGG{$^{+}$}97}
\bibcite{rice2006mathematical}{Ric06}
\bibcite{rising2015efficient}{RKT15}
\bibcite{robert1999metropolis}{Rob99}
\bibcite{roberts2009examples}{RR09}
\bibcite{ruschendorf2014mathematische}{R{\"u}s14}
\bibcite{samuel1959some}{Sam59}
\bibcite{tao2010universality}{Tao10}
\bibcite{tauchen1985diagnostic}{Tau85}
\bibcite{urschel2017learning}{UBMR17}
\bibcite{vavasis1991nonlinear}{Vav91}
\bibcite{vavasis1995complexity}{Vav95}
\bibcite{10.2307/1967604}{Veb12}
\bibcite{volkenstein2009entropy}{Vol09}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{113}{chapter*.19}}
\global\@altsecnumformattrue
