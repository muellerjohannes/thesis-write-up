{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf400
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww28600\viewh18000\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs36 \cf0 Proposed Structure:\
\
	Abstract\
	I. Introduction\
		\'97 Motivation\
		\'97 Previous work\
		\'97 Aim and outline of the dissertation\
	II. DPPs: Basics\
		\'97 Historical remarks\
		\'97 Definitions and properties:\
			+ Definition of DPPs\
			+ L-ensembles\
			+ quality diversity decomposition\
		\'97 Related structures\
			+ conditional DPPs: explain how they can be used to extrapolate knowledge of some DPPs to other\
			+ structured DPPs: explain how they can drastically reduce the amount of parameters of the model\
			+ DPPs of fixed size or k-DPPs\
		\'97 computational benefits (\'82Magic properties  of DPPs\'92)\
			+ Closed formulae for a lot of expressions\
			+ Sampling algorithm\
		\'97 Problem of mode selection\
			+ give motivation for the mode search\
			+ explain difficulties in the DPP case\
			+ present different approaches\
	III. Learning\
		\'97 What does \'82learning\'91 mean and why is it interesting?\
		\'97 Estimation of the marginal kernel using principle minors\
			+ Main Idea: estimate principle minors over the marginal density and reconstruct the matrix from its principle minors\
			+ Question: Is this estimator unbiased? Is it consistent? (It should be consistent in some way by the result from the paper)\
		\'97 Maximum likelihood estimation using optimisation techniques\
			+ Learning the kernel L\
				Problem: Non concavity of the log likelihood function\'85\
				http://proceedings.mlr.press/v37/mariet15.pdf uses a fixed point iteration to approximate stationary points\
			+ Learning the quality\
				Log likelihood becomes concave\
				Questions: In general the maximiser does not have to exist, so how can one deal with this? Is the estimator consistent?\
			+ Learning the mixture coefficients of k-DPPs\'85\
		\'97 Bayessian approach to the kernel estimation: It looks like that has been done in www.jmlr.org/proceedings/papers/v32/affandi14.pdf			+ Explain the Bayssian approach to parameter estimation\
	IV. Examples\
		\'97 Points on the line, including duality to 0-1 sequences that look like human generated pseudo random ones		\'97 Points in the square, that\'92s more or less \'82playing Battleships\'91\
		\'97 Toy example for learning\
	V. Summary and Conclusion\
	Appendix\
		\'97 code\
\
\
\
IGNORE FROM HERE ON PLEASE! :)\
\
Questions:\
	\'97 Length of thesis? Formal requirements?\
	\'97 Bring in some more maths?\
	\'97 Own contributions?\
	\'97 Consistency of estimators under loss functions?\
	\'97 coercivity, semi continuity and convexity of the loss function? does a log linear likelihood approach lead directly to a log linear quality?\
	\'97 How can one learn the parameter sigma?!\
	\'97 Non symmetric kernels\
\
Further repulsive structures:\
	\'97 Locations of cities/supermarkets/gas stations etc\
	\'97 \
}