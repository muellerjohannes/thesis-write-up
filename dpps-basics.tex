\chapter{Determinantal point processes: Basic notions and properties}

In this chapter we provide an overview over the basic notions and results for discrete determinantal point processes. Those will be necessary to study the problem of parameter estimation later on. First we rigorously introduce the concept of discrete determinantal point processes and define the most important subclass that we will work with later. This is exactly the subclass where one can express the elementary probabilities of the point process nicely. This will be crucial later on if one wants to perform the parameter estimation based on elementary probabilities, like for example maximum likelihood estimation or Bayesian parameter estimation.

Then we will turn towards the question of existence and simulation of determinantal point processes. This will lead to an algorithm that samples from a DPP which we will use to simulate toy examples and also to generate the data sets we will perform parameter estimation for in the following chapters.

\section{Definitions and properties}

We begin by presenting the general frame we will work in. This means that we will keep the notation introduced here and will use those objects throughout the thesis without further explanation. We present all the important properties of determinantal point processes that we will need but omit some calculations that have been presented somewhere else and don’t contribute to a better understanding of the topic. A much more in depth survey of properties of determinantal point processes including an extensive comparisons to several other point processes can be found in the report \cite{kulesza2012determinantal}.

\begin{emp}[Setting]
Let \(\mathcal Y\) be a finite set, which we call the \emph{ground set} and \(N\coloneqq \left\lvert \mathcal Y \right\rvert\) its cardinality. We call the elements of \(\mathcal Y\) \emph{items } and assume for the sake of easy notation \(\mathcal Y = \left\{ 1, \dots, N\right\}\) unless specified differently. A \emph{point process} on \(\mathcal Y\) is a random subset of \(\mathcal Y\), i.e. a random variable with values in the powerset \(2^{\mathcal Y}\). We will identify this random variable with its law\footnote{The law of a random variable is the push forward measure of the probability measure of the probability space the random variable is defined on.} \(\mathbb P\) and thus refer to probability measures \(\mathbb P\) on \(2^{\mathcal Y}\) as point processes and will not distinguish between those objects. Further, \(\mathbf Y\) will always denote a random subset distributed according to \(\mathbb P\).
\end{emp}

\begin{defi}[Determinantal point process]
We call \(\mathbb P\) a \emph{determinantal point process}, or in short a \emph{DPP}, if we have
\begin{equation}\label{e2.1}
\mathbb P(A\subseteq \mathbf Y) = \det(K_A) \quad \text{for all } A\subseteq \mathcal Y
\end{equation}
where \(K\) is a symmetric matrix indexed by the elements in \(\mathcal Y\) and \(K_A\) denotes the submatrix 
\((K_{ij})_{ij\in A}\)
of \(K\) indexed by the elements of \(A\). We call \(K\) the \emph{marginal kernel} of the DPP. If the marginal kernel \(K\) is diagonal, we call \(\mathbb P\) a \emph{Poisson point process}.
\end{defi}

We note that all principal minors\footnote{The \emph{principal minors} of \(K\) are the determinants of the submatrices \(K_A\) for \(A\subseteq \mathcal Y\).} of \(K\) are necessarily non negative and Sylvester’s criterion implies that \(K\) is positive semi-definite.\footnote{\(K\) is called \emph{positive semi-definite} if \(x^TKx\ge0\) for all \(x\in\mathbb R^{\mathcal Y}\). The Sylvester criterion states that a matrix is positive semi-definite if and only if all principal minors are non negative.} Further it can be shown \footnote{This follows from equation (2.3) in \cite{borodin2009determinantal}.} that also the complement of a DPP is a DPP with marginal kernel \(I-K\) where \(I\) is the identity matrix, i.e.
\[\mathbb P(A\subseteq\mathbf Y^c) = \det(I_A-K_A).\]
Thus, we can conclude \(I-K\ge0\) and obtain \(0\le K\le I\). This actually turns out to be sufficient for \(K\) to define a DPP through \eqref{e2.1} which we will see in the fourth section of this chapter.

\begin{emp}[Repulsive behaviour of DPPs]
If we choose \(A=\left\{ i\right\}\) and \(A=\left\{ i,j\right\}\) for \(i,j\in \mathcal Y\) in \eqref{e2.1} we obtain the probabilities of the occurrence of the items \(i\) and \(j\)
\begin{equation}\label{e2.2}
\begin{split}
\mathbb P(i\in \mathbf Y) & = K_{ii} \quad \text{and} \\
\mathbb P(i,j\in\mathbf Y) & = K_{ii}K_{jj}-K_{ij}^2 = \mathbb P(i\in\mathbf Y)\cdot\mathbb P(j\in\mathbf Y)-K_{ij}^2.
\end{split}
\end{equation}
Thus, the appearances of the two items \(i\) and \(j\) are always negatively correlated. This negative correlation is exactly what causes the diversifying behaviour of determinantal point processes and leads to a repulsive behaviour that can be seen in Figure \ref{fig:1}. Note that Poisson point processes are exactly the DPPs without correlations of the points.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.444\textwidth]{figures/DPP-in-square-large-3}
	\includegraphics[width=0.44\textwidth]{figures/Poisson-in-square-large-2}
	\caption{A DPP with negative correlations of close points on a \(100\times100\) grid in the unit square on the left and a Poisson point process on the same grid and with the same expected cardinality on the right. The -- in this case spatially -- repellent structure of the DPP is clearly visible.}
	\label{fig:1}
\end{figure}

In this light the fact that also \(\mathbf Y^c\) exhibits negative correlations becomes less surprising. Since the set \(\mathbf Y\) tends to spread out due to the repulsion in \eqref{e2.2}, the complement, which is nothing but the gaps that are left after eliminating the elements in \(\mathbf Y\), tend to show a repulsive structure too.
\end{emp}

\begin{emp}[\(L\)-ensembles]
Let us now introduce an important subclass of DPPs, namely the ones where not only the marginal but also the elementary probabilities can be expressed through a suitable kernel. This will be convenient for us later when we will need expressions for the elementary probability in order to take a maximum likelihood approach\footnote{This will thoroughly be introduced in the next chapter.} to the estimation of certain parameters. If we have \(K<I\), then we define the \emph{elementary kernel}
\begin{equation}\label{e2.2.1}
L\coloneqq K(I-K)^{-1}
\end{equation}
which specifies the elementary probabilities since one can check \footnote{This is done in full detail in \cite{kulesza2012learning} and we will not repeat those arguments here.}
\begin{equation}\label{e2.3}
\mathbb P(A=\mathbf Y) = \frac{\det(L_A)}{\det(I+L)} \quad\text{for all } A\subseteq\mathcal Y.
\end{equation}
Conversely for any \(L\ge0\) a DPP can be defined via \eqref{e2.2} and the corresponding marginal kernel is given by the inversion of \eqref{e2.2.1}
\[K=L(I+L)^{-1}\]
and we have again \(K<I\). We call DPPs which arise this way \(L\)-\emph{ensembles}. We will see in \ref{carDPP} that the cardinality of a DPP is distributed like the sum of \(N\) Bernoulli experiments with expectation \((\lambda_n)_{n=1, \dots, N}\) where \(\lambda_n\) are the eigenvalues of \(K\). Being an \(L\)-ensemble is equivalent to \(K<I\) which again is equivalent to \(\lambda_n<1\) for all \(n=1, \dots, N\) and hence equivalent to
\[\mathbb P(\mathbf Y = \varnothing) = \mathbb P(\left\lvert \mathbf Y \right\rvert = 0) > 0.\]
\end{emp}

\subsection*{The quality diversity decomposition}

We note that any symmetric positive semi-definite matrix \(L\) can be written as a Gram matrix
\[L=B^TB\]
where \(B\in\mathbb R^{D\times N}\) and \(D\) is larger than the rank \(\operatorname{rk}(L)\) of \(L\). For example one could take the spectral decomposition \(L = U^TCU\) of \(L\) and set \(B\coloneqq \sqrt{C} U\) and eventually drop some zero rows from \(\sqrt{C}\). Let \(B_i\) denote the \(i\)-th column of \(B\) and write this as a product \(B_i = q_i\cdot \phi_i\) where \(q_i\ge0\) and \(\phi_i\in\mathbb R^D\) such that \(\left\lVert \phi_i \right\rVert=1\). This yields the representation
\[L_{ij} = q_i \phi_i^T\phi_j q_j \eqqcolon q_i S_{ij}q_j\]
and we call \(q_i\) the \emph{quality} of the item \(i\in \mathcal Y\) and \(\phi_i\) the \emph{diversity feature vector} of \(i\) and \(S\) the \emph{similarity matrix} or \emph{similarity kernel}. Since we will use this decomposition multiple times, we fix its properties.

\begin{prop}[Quality diversity parametrisation]
Let \(D\in\mathbb N\) and let \(\mathbb S_D\) denote the sqhere in \(\mathbb R^D\). 
Further let \(\mathbb R^{N\times N}_{\text{sym}, +}\) be the set of symmetric positive semi-definite \(N\times N\) matrices. 
The quality diversity parametrisation is a continuous and surjective mapping
\[\Psi\colon\mathbb R_+^N\times \mathbb S_D^N \to \left\{L\in \mathbb R^{N\times N}_{\text{sym}, +} \;\big\lvert\; \operatorname{rk}(L)\le D \right\}, \quad (q, \phi) \mapsto \left( q_i\phi_i^T\phi_j q_j\right)_{1\le i, j\le N}. \]
\end{prop}

\begin{rem}
\begin{enumerate}
\item In the case \(D \ge N\) the quality diversity decomposition gives a parametrisation of the whole symmetric positive semi-definite \(N\times N\) matrices.
\item Note that this parametrisation is not unique, i.e. \(\Psi\) is not injective. For example the identity matrix \(I\) can be parametrised by any orthonormal system \(\phi\in\mathbb S_N^N\) and \(q = (1, \dots, 1)^T\).
\item One can without any problems consider diversity features \(\phi_i\) in an abstract Hilbert space \(\mathcal H\). However, we will not need this in the remainder and thus restrict ourselves to the easier case of Euclidean diversity features.
\item We call every preimage \((q, \phi)\) of \(L\) under \(\Psi\) \emph{quality diversity decomposition} of \(L\). Further, we call the tupel \(\phi\in \mathbb S_D^N\) of normalised vectors the \emph{diversity feature matrix}.
\end{enumerate}
\end{rem}


Not only will the quality diversity decomposition play a central role when it comes to the modelling of real world phenomena with DPPs. It will also provide some useful expressions like the following one for the elementary probabilies
\begin{equation}\label{e2.4}
\mathbb P(A=\mathbf Y) \propto \det\!\big((B^TB)_A\big) = \left(\prod_{i\in A} q_i^2\right) \cdot \det(S_A) \quad \text{for all } A\subseteq\mathcal Y.
\end{equation}

In order to get an intuitive understanding of the quality diversity decomposition we can think of \(q_i\ge0\) as a measure of how important or high in quality the item \(i\) is and the diversity feature vector \(\phi_i\in\mathbb R^D\) can be thought of as some kind of state vector that consists of internal quantities that describe the item \(i\) in some way. Further, we interpret the scalar product \(\phi_i^T\phi_j\in[0,1]\) as a measure of similarity between the items \(i\) and \(j\) which justifies the name similarity matrix for \(S\). Note that if \(i\) and \(j\) are perfectly similar or antisimilar, i.e. \(\phi_i^T\phi_j=\pm1\), then they can not occur at the same time, since
\[\mathbb P(i,j\in\mathbf Y) = \det\begin{pmatrix} 1 & \pm1 \\ \pm1 & 1 \end{pmatrix} = 0. \]
If we identify \(i\) with the vector \(B_i=q_i\phi_i\in\mathbb R^D\), we can obtain a geometric interpretation of \eqref{e2.4} since \(\det\!\big((B^TB)_A\big)\) is the squared volume that is spanned by the columns \(B_i, i\in A\), which is visualised in \ref{fig:2}. This volume increases if the lengths of the edges which correspond to the quality increase and decrease when the similarity feature vectors point into more similar directions.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/qd-decomposition}
	\caption{The first line (a) illustrates the volumes spanned by vectors, and in the second line it can be seen how this volume increases if the length -- associated with the quality -- increases (b) and decreases if they become more similar in direction which we interpret as two items becoming more similar (c). Original graphic from \cite{kulesza2012determinantal}.}
	\label{fig:2}
\end{figure}

\begin{emp}[Modelling diversity over distance]\label{moddiv}
Since we will use one approach for the diversity features multiple times, we will now give a short general formulation of it. Let \(\mathcal R = \left\{ r_1, \dots, r_D\right\}\) be a finite set which we will call the \emph{reference set} and its elements the \emph{reference points}. Further, let
\[d\colon \mathcal Y\times \mathcal R\to\mathbb R_+, \quad f\colon\mathbb R_+\to \mathbb R\]
be mappings. Usually \(d(i, r)\) will be some measure of distance between an item \(i\in\mathcal Y\) and a reference point \(r\in\mathcal R\) and will typically be given by a metric on a larger space that contains \(\mathcal Y\) and \(\mathcal R\).
One can now model \(\phi_i\in\mathbb R^{\mathcal R}\) via
\[(\phi_i)_r \propto f\!\big(d(i, r)\big) \quad \text{for } r\in\mathcal R.\] 
The function \(f\) will typically be decreasing and thus \((\phi_i)_r\) can be seen as a measure of how similar item \(i\) is to the reference point \(r\in\mathcal R\). Thus, the diversity feature vector \(\phi_i\) encodes how similar the item \(i\) is to all reference points and the scalar product \(\phi_i^T\phi_j\) will be close to one, if the items \(i\) and \(j\) have approximately the same degrees of similarity to the reference points. It shall be noted that the choice of the number of reference points bounds the rank of the kernel \(L\) and therefore also the largest subset that occurs with positive probability. Indeed we have \(\operatorname{rk}(L)\le D\) and for \(A\subseteq\mathcal Y\) with more than \(D\) elements \(\det(L_A) = 0\) and therefore \(\mathbb P(A) = 0\).
In the last section of this chapter we will give examples where \(d\) is quite naturally a metric and will see how the choice of \(f\) is crucial for the strength of the repulsion.

Similar approaches for the modelling of the diversity feature vectors have been taken in \cite{leeindividualness} and \cite{kulesza2010structured} and further the method of reference points has been used in \cite{bardenet2015inference} to obtain bounds for the elementary probabilities of a DPP.
\end{emp}

\begin{emp}[Comparison to other point processes]
A wide variety of point processes has been studied and used in different applications and determinantal point processes are by far not the only diversifying point processes. For example every Poisson point process can be turned into a process with negative correlations by removing  all points that lie within a certain distance of another point of the subset. Another well studied class of point processes are the so called \emph{Gibbs} or \emph{Markov point proccesses}. The elementary probabilities are given by 
\[\mathbb P(A) \propto \exp(-F(A))\]
where \(F\colon2^{\mathcal Y}\to\mathbb R\) is called the \emph{energy function} is interpreted as a measure of how unfavourable a subset \(A\subseteq\mathcal Y\) is.

Although some of those classes of point processes posses nice theoretical properties they share one major drawback. In fact, a lot computations and also the simulation of those point processes can not be performed efficiently. For example, in the case of Gibbs point processes even the time needed for the computation of the normalisation constant
\[\sum_{A\subseteq\mathcal Y} \exp(-F(A)) \]
grows exponentially with \(N\) since it is the sum over an exponentially large set. However, the special structure of the determinant itself leads to the explicit expression in \eqref{e2.3} of the normalisation constant for DPPs and the computation time for the exact computation of it via Gauss elimination only grows like \(N^3\) or even slower for numerical approximations (cf. \cite{valiant1979complexity} and \cite{emiris2005improved}). A more in depth comparison of different point processes including their descriptive power can be found in \cite{kulesza2012determinantal}.
\end{emp}

\subsubsection*{The mode problem}

One general motivation for modelling is the hope that one can make predictions based on the selected model. If the model is of stochastic nature, like in our case, and if one wants to predict its outcome, there are a few possible approaches. The first and possibly simplest one would be to sample from this model. This relies on the intuition that a realisation of a random variable should be a rather typical example for the random event. Going one step further one could try to find the most likely outcome of the random variable, which is known as the mode problem.

\begin{emp}[The mode problem]
Let \(X\) be a random variable with values in some space \(\mathcal X\) and let \(f\) be the density of the distribution of \(X\) with respect to some reference measure. Then the \emph{mode} is the maximiser
\[\hat x = \underset{x\in\mathcal X}{\arg\max}f(x)\]
of the density if it exists. The search for the mode is called the \emph{mode problem}.
\end{emp}


Our motivation for finding the mode of a random variable was to make better predictions for it. This hope is based on the believe that the mode should be a typical realisation of the random variable. However, this is not generally the case and therefore one should be cautious with this intuition.
To see this we consider a random natural number with the following distribution
\[\mathbb P(\left\{ n\right\}) \coloneqq \begin{cases} \;0.1 \quad & \text{if } n = 20 \\\; 0.09 & \text{if } n = 0, 1, \dots, 9 \\\; 0 &\text{otherwise} \end{cases}. \]
Although \(20\) is the most likely elementary event, it is not a very typical outcome, since in \(90\%\) of the cases the random variable will have values in \(\left\{ 0, 1, \dots, 9\right\}\) and hence is far away from the mode of the distribution. Similar examples can easily be constructed for continuous distributions.

The mode problem can often be solved explicitely or at least numerically if the density \(f\) is a smooth function defined on a subset of \(\mathbb R^d\). However, in the case of DPPs we have to deal with the probability measure on a finite set and thus the mode problem is a discrete optimisation problem over the powerset \(2^{\mathcal Y}\). The exponential size of the powerset turns this into a hard computational task and it has been shown that the time to compute the mode -- or even an event with more than \(\frac89\) its probability -- grows exponentially with the cardinality \(N\) of \(\mathcal Y\) (cf. \cite{kulesza2012determinantal}). However, some algorithms for the approximation of the mode have been proposed for certain classes of DPPs or for the goal to find a subset of at least \(\frac14\) of the probability of the mode. For further reading we refer to \cite{djolonga2014map} and \cite{gillenwater2012near}.

\section{Variations of DPPs}

In this section we will present some useful variations of determinantal point processes. They serve different purposes and we will shortly explain their individual benefits. 

\begin{emp}[Conditional DPPs]
A \emph{conditional DPP} is a collection of DPPs indexed by \(X\in\mathcal X\), where \(X\) is called the \emph{input} of the conditional DPP. Thus, for every \(X\in\mathcal X\) we get a finite set \(\mathcal Y(X)\) and a determinantal point process \(\mathbb P(\cdot| X)\) on \(\mathcal Y(X)\). We will usually assume that the single DPPs are \(L\)-ensembles and hence they are given by elementary kernels \(L(X)\), i.e.
\[\mathbb P(A\lvert X) \propto \det\left(L_A(X)\right) \quad \text{for all } A\subseteq\mathcal Y(X). \]
Further we denote the quality and diversity features of the conditional DPP by \(q_i(X)\) and \(\phi_i(X)\) respectively.

It is not immediately clear why one would want to model a family of DPPs as a conditional DPP rather than as separate DPPs. The reason for this is that one wants to estimate the kernels \(L(X)\) for every \(X\in \mathcal X\). With a naive approach one would need to observe each of the DPPs \(\mathbb P(\cdot| X)\) individually which is often not possible. Thus, one hopes to not only memorise the kernels \(L(X)\) for every single input \(X\in\mathcal X\) but rather to estimate the mapping that assigns every input \(X\) its elementary kernel \(L(X)\). If one achieved this task, one would be able to simulate and predict a DPP that one has not observed so far just by the knowledge about some DPPs that belong to the same conditional DPP. Of course this can only work if we assume some regularity or a certain structure of the function \(L\) and we will see one approach how this can be done in the next chapter.

In conclusion conditional DPPs are suitable for the extrapolation of parameter estimation from observed to similar DPPs.
\end{emp}

\begin{emp}[Fixed size or \(k\)-DPPs]
We have introduced DPPs as a model of random diverse subsets of a finite set. However, there are a lot of cases where the size of this subset is already known, like for example if the DPP models the position of football players on the field, we already know how many points we have to select, namely 11 -- at least if no player was sent off or got injured.

The straight forward procedure to obtain a probability distribution over all subsets of a fixed size that still propagates diversity is to condition a DPP on the event that it has this exact size. If we conditioned on the event that the point process has \(k\le N\) elements, we call this new point process \emph{\(k\)-DPP}. Luckily \(k\)-DPPs possess similarly attractive properties like normal DPPs, in the sense that there is an analytical form of the normalisation constant as well as an effective sampling algorithm (cf. \cite{kulesza2011k}). Hence, \(k\)-DPPs allow to describe random diverse subsets of fixed size. 
\end{emp}

\begin{emp}[Structured DPPs]
We call a DPP \emph{structured DPP} or short \emph{sDPP} if the ground set is the cartesian product of some other set \(\mathcal M\), which we will call the \emph{set of parts}, i.e. if we have
\[\mathcal Y = \mathcal M^R = \Big\{ y_i = (y_i^r)_{r = 1, \dots, R} \in\mathcal M^R\;\big\lvert\; i = 1, \dots, N\Big\}\]
where \(R\) is a natural number, \(M = \left\lvert \mathcal M \right\rvert\) and \(N = M^R\). The quality diversity decomposition of \(L\) take the form 
\[L_{ij} = q(y_i) \phi(y_i)^T \phi(y_j) q(y_j)\]
and since \(N = M^R\) is typically very big, it is impractical to define or store the quality and diversity features for every item \(y_i\in\mathcal Y\). To deal with this problem we will assume that they admit factorisations and are thus a combination of only a few qualities and diversities.

More precisely, we call \(F\subseteq 2^{\left\{ 1, \dots, R\right\}}\) a \emph{set of factorisations} and for a \emph{factor} \(\alpha\in F\), \(y_\alpha\) denotes the subtupel of \(y\in\mathcal Y\) that is indexed by \(\alpha\). Further, we will work with the decompositions
\begin{equation}
\begin{split}
q(y) = & \prod_{\alpha\in F}q_\alpha(y_\alpha) \\
\phi(y) = & \sum_{\alpha\in F} \phi_\alpha(y_\alpha)
\end{split}
\end{equation}
for a suitable set of factorisations \(F\) and qualities and diversities \(q_\alpha\) and \(\phi_\alpha\) for \(\alpha\in F\). Note that so far this is neither a restriction of generality -- we could simply choose \(F = \big\{\!\left\{ 1, \dots, R\right\}\!\big\}\) -- nor a simplification -- in that case we have the exact same number of qualities and diversities. However, we are interested in the case where \(F\) consists only of small subsets of \(\left\{ 1, \dots, R\right\}\). For example, suppose that \(F\) is the set of all subsets with one or two elements, then we only have\footnote{We write \(f(x) = O(g(x))\) if \(f(x) \le M g(x)\) for all \(x\ge x_0\) and one \(M>0\).}
\[R\cdot M + \binom{R}{2} \cdot M^2 = O(R^2M^2)\]
quality and diversity features instead of
\[M^R = O(M^R).\]
This reduction of variables will make modelling, storing and estimating them  possible again in a lot of cases where naive approaches are foredoomed because of their shear size.

Because we will neglect sDPPs in the following, we should quickly mention the reason why one could want to select the set with one and two elements as a factorisation. One could try to describe the trajectory of football players over a field through a sDPP and hence \(\mathcal M\) would be a discretisation of the field and \(r = 1, \dots, R\) the different timesteps that one considers. Then the qualities \(q_\alpha\) for \(\left\lvert \alpha \right\rvert = 1\) are a measure of how favourable a position is for a player and the qualities \(q_\alpha\) for \(\left\lvert \alpha \right\rvert = 2\) can be seen as \emph{transition qualities} that encode how good the transition from one position to another is. This gives the opportunity to dictate a certain regularity to the paths since a very big jump in position -- the equivalent to a very irregular path -- is very unlikely to occur in real life and can therefore be made unlikely by the assignment of a low transition quality. For more examples of the versatile applications of sDPPs we refer to \cite{kulesza2010structured}.
\end{emp}



\section{Simulation and Existence of DPPs}

One of the greatest challenges in the application of discrete point processes is that they are probability measures over an exponentially large set, namely the powerset \(2^{\mathcal Y}\) which has cardinality \(2^N\). Determinantal point processes have the benefit that they describe this distribution through the matrix \(K\) which consists of only \(N^2\) parameters. This reduction of the number of parameters plays a central role in making a lot of operations possible in a computationally efficient way. However, it is not only the relatively small amount of parameters that lead to this, but also the structure of the determinant itself that leads to analytical expressions for a lot of quantities like the normalisation constant in \eqref{e2.3}.
In this section we will focus on the simulation of DPPs and see how the special properties of the determinant play a central role here as well. In the end we will give a short overview of further techniques that can improve the performance of this algorithm.

It should be mentioned that this section can be skipped if one is solely interested in the estimation of the parameters of DPPs.

\subsection{Cauchy-Binet type identities}

First we state a general form of the famous Cauchy-Binet identity and will then derive the version for matrices afterwards. Then we derive a result which can be seen as a formula for marginalisation for determinantal point processes and adapt ideas from \cite{rezakhanlou2012lectures} for this.

\begin{prop}[Cauchy-Binet]
Let \((\mathcal X, \mu)\) be a \(\sigma\) finite measure space and let further \(\phi_i, \psi_i\in L^2(\mu)\) be square integrable functions for \(i = 1, \dots, n\). Then we have
\begin{equation*}
\begin{split}
\frac1{n!} \int\limits_{\mathcal X^n} & \det\left( \phi_i(x_j)\right)_{1\le i, j\le n}\det\left( \psi_i(x_j)\right)_{1\le i, j\le n} \mu(\mathrm dx_1)\cdot \ldots\cdot \mu(\mathrm dx_n) \\
 & = \det\left( \int_{\mathcal X} \phi_i(x)\psi_j(x)\mu(\mathrm dx)\right)_{1\le i, j\le n}.
\end{split}
\end{equation*}
\end{prop}
\begin{proof}
We use the Leibniz formula to express the determinants in terms of permutations. This yields
\begin{equation*}
\begin{split}
& \int\limits_{\mathcal X^n} \det\left( \phi_i(x_j)\right)_{1\le i, j\le n}\det\left( \psi_i(x_j)\right)_{1\le i, j\le n} \mu(\mathrm dx_1)\cdot\ldots\cdot\mu(\mathrm dx_n) \\
= & \; \int\limits_{\mathcal X^n} \sum_{\sigma, \tau\in S_n} \operatorname{sgn}(\sigma)\operatorname{sgn}(\tau) \prod_{i=1}^n \phi_i(x_{\sigma(i)})\psi_i(x_{\tau(i)}) \mu(\mathrm dx_1)\cdot\ldots\cdot\mu(\mathrm dx_n) \\
= & \; \int\limits_{\mathcal X^n} \sum_{\sigma, \tau\in S_n} \operatorname{sgn}(\sigma)\operatorname{sgn}(\tau) \prod_{i=1}^n \phi_i(x_{\sigma(i)})\psi_{\tau^{-1}(\sigma(i))}(x_{\sigma(i)}) \mu(\mathrm dx_1)\cdot\ldots\cdot\mu(\mathrm dx_n) \\
= & \; \sum_{\sigma, \tau\in S_n} \operatorname{sgn}\big(\tau^{-1} \circ\sigma\big) \prod_{i=1}^n \int\limits_{\mathcal X} \phi_i(x) \psi_{\tau^{-1}(\sigma(i))}(x)\mu(\mathrm dx) \\
= & \; n!\cdot \sum_{\rho\in S_n} \prod_{i=1}^n \int\limits_{\mathcal X} \phi_i(x) \psi_{\rho(i)}(x)\mu(\mathrm dx) \\
= & \; n!\cdot\det\left( \int_{\mathcal X} \phi_i(x)\psi_j(x)\mu(\mathrm dx)\right)_{1\le i, j\le n}.
\end{split}
\end{equation*}
In the calculation we have used that the sign function is a group homomorphism from the permutation group to \(\left\{ \pm1\right\}\) and thus
\[\operatorname{sgn}\big(\tau^{-1}\circ\sigma\big) = \operatorname{sgn}(\tau)^{-1} \operatorname{sgn}(\sigma) = \operatorname{sgn}(\sigma)\operatorname{sgn}(\tau). \]
Further the second to last step is valid since for \(\rho\in S_n\) exactly \(n!\) pairs of permutations \((\sigma, \tau)\) satisfy \(\tau^{-1}\circ\sigma = \rho\).
\end{proof}

Now we present a discrete analogon of the Cauchy-Binet identity which will be of great use later. We write \([n]\) for the set \(\left\{ 1, \dots, n\right\}\) where \(n\) is a natural number and \(A_{IJ}\) for the submatrix of \(A\) where the first index is in \(I\) and the second one in \(J\). Further, we keep the notation \(A_I = A_{II}\).

\begin{prop}[Cauchy-Binet for matrices]
Let \(m, n \in\mathbb N, m\le n\) be two natural numbers and \(A\in\mathbb R^{m\times n}, B\in\mathbb R^{n\times m}\) two matrices.  Then we have
\[\det(AB) = \sum_{\begin{subarray}{c} I\subseteq[n] \\ \left\lvert I \right\rvert = m \end{subarray}} \det\left(A_{[m]I}\right) \det\left(B_{I[m]}\right). \]
\end{prop}
\begin{proof}
The assertion follows from the general Cauchy-Binet identity by using the counting measure on \([n]\) since the right hand side is equal to
\[ \frac1{m!}\sum_{i_1, \dots, i_m\in[n]} \det\left(A_{ki_l}\right)_{1\le k, l\le m} \det\left(B_{i_kl}\right)_{1\le k, l\le m} \]
where we used that the determinants vanish if two indices \(i_k\) and \(i_l\) agree for \(k\ne l\).
\end{proof}


\begin{prop}[Marginalisation]
Let \((\mathcal X, \mu)\) be a a \(\sigma\) finite measure space and assume that \(\left\{ \phi_i\right\}_{i=1, \dots, n}\subseteq L^2(\mu)\) is an orthonormal set. Let \(x_1, \dots, x_m\in\mathcal X\) for \(m<n\), then we have
\begin{equation*}
\begin{split}
\frac{1}{(n-m)!} \int\limits_{\mathcal X^m} & \det\left( \phi_i(x_j)\right)_{1\le i, j\le n}^2 \mu(\mathrm dx_{m+1})\cdot \ldots\cdot \mu(\mathrm dx_n) \\
 & = \det\left( \sum_{k=1}^n \phi_k(x_i)\phi_k(x_j)\right)_{1\le i, j\le m}.
\end{split}
\end{equation*}
\end{prop}
\begin{proof}
Just like in the proof of the Cauchy-Binet identity we begin by expressing the determinants through permutations and obtain
\begin{equation*}
\begin{split}
& \int\limits_{\mathcal X^m} \det\left( \phi_i(x_j)\right)_{1\le i, j\le n}^2 \mu(\mathrm dx_{m+1})\cdot \ldots\cdot \mu(\mathrm dx_n) \\
= & \sum_{\sigma, \tau\in S_n} \operatorname{sgn}(\sigma)\operatorname{sgn}(\tau) \int\limits_{\mathcal X^m} \prod_{k=1}^n \phi_{\sigma(k)}(x_k) \phi_{\tau(k)}(x_k) \mu(\mathrm dx_{m+1}) \cdot\ldots\cdot\mu(\mathrm dx_n). 
\end{split}
\end{equation*}
The multiple integrals over the product can be evaluated individually and hence the term above is only non trivial -- and identical to one in this case -- if \(\sigma(k) = \tau(k)\) for \(k=m+1, \dots, n\) which we will denote by \(\sigma\sim\tau\). Therefore, the expression is equal to
\begin{equation*}
\begin{split}
& \sum_{\begin{subarray}{c} \sigma, \tau\in S_n \\ \sigma\sim\tau \end{subarray}} \operatorname{sgn}(\sigma)\operatorname{sgn}(\tau) \prod_{k=1}^m \phi_{\sigma(k)}(x_k) \phi_{\tau(k)}(x_k) \\
= & \; (n-m)!\cdot \sum_{\begin{subarray}{c} I\subseteq[n] \\ \left\lvert I \right\rvert = m \end{subarray}} \det\left( \phi_{i_k}(x_l)\right)_{1\le k, l\le m}^2 \\
= &\; (n-m)!\cdot \det\left( \sum_{k=1, \dots, n} \phi_k(x_i)\phi_k(x_j)\right)_{1\le i, j\le m}
\end{split}
\end{equation*}
where we used Cauchy-Binet and the notation \(I = \left\{ i_1, \dots, i_m\right\}\). The combinatorial factor of \((n-m)!\) arises since for a fixed image \(I = \sigma(\left\{ 1, \dots, m\right\})\), the two permutations \(\sigma\) and \(\tau\) still have \((n-m)!\) possibilities for their values on \(\left\{ m+1, \dots, n\right\}\).
\end{proof}

Just like in the case of the Cauchy-Binet identity we will give a discrete version of the previous result in which the normalisation factor does not appear.

\begin{prop}[Variation of Cauchy-Binet]\label{CB-type}
Let \(m \le n\) be two natural numbers and \(B\in\mathbb R^{m\times n}\) be a matrix such that the rows of \(B\) form an orthonormal system. Further, let \(I \subseteq[n]\) with \(\left\lvert I \right\rvert\le m\), then we have
\begin{equation*}
\begin{split}
\det\left( (B^TB)_{I}\right) = \sum_{\begin{subarray}{c} I\subseteq J\subseteq[n] \\ \left\lvert J \right\rvert = m \end{subarray}} \!\!\! \det\left(B_{[m]J}\right)^2.
\end{split}
\end{equation*}
\end{prop}
\begin{proof}
Set \(r\coloneqq \left\lvert I \right\rvert \le m\). If \(r=m\), then the statement is trivial. Otherwise, let \(I = \left\{ i_1, \dots, i_r\right\}\), then the right hand side is equal to
\begin{equation}\label{CBP}
 \frac1{(m-r)!} \cdot \sum_{i_{r+1}, \dots, i_m\in[n]} \det\left( B_{ki_l} \right)_{1\le k, l\le r}^2
\end{equation}
where we used that the determinant vanishes if two indices \(i_k\) and \(i_l\) agree for \(k\ne l\). Now the previous result completes the proof.
\end{proof}

\subsection{Sampling and Existence}

We roughly follow the approaches taken in \cite{hough2006determinantal} and \cite{kulesza2012determinantal} and will start with the result that every determinantal point process is the mixture of a smaller class of DPPs.

\begin{theo}[Mixture representation of DPPs]\label{mixDPPs}
Let \(\mathbb P\) be a DPP and \[K = \sum_{k=1}^N \lambda_kv_kv_k^T\] be the spectral decomposition of its marginal kernel. Let now \(\left\{ \xi_k\right\}_{k =1, \dots, N}\) be a collection of independent Bernoulli random variables with mean \(\lambda_k\). Define now the random kernel
\begin{equation}\label{BerKer}
K_\xi = \sum_{k=1}^N \xi_k v_kv_k^T.
\end{equation}
Finally, define a second point process \(\tilde{\mathbb P}\) on \(\mathcal Y\) that is obtained by first drawing the Bernoulli variables \(\xi_k\) and then a DPP according to \(K_\xi\). Then we have \(\tilde{\mathbb P} = \mathbb P\) and thus \(\tilde{\mathbb P}\) is also a DPP with marginal kernel \(K\).
\end{theo}

We will postpone the proof and first discuss its consequences which will be the existence of DPPs for a given marginal kernel as well as the construction of a sampling algorithm.

\begin{rem}
Since it is fairly easy to simulate Bernoulli experiments, it remains to know how we can sample from DPPs with marginal kernels of the form \(K=\sum_{k=1}^m v_kv_k^T\) for some \(m\le N\). We call DPPs of this type \emph{elementary} and note that this corresponds to the class of DPPs where the eigenvalues of the marginal kernel are contained in \(\left\{ 0, 1\right\}\).
\end{rem}

Now we study the existence and simulation of elementary DPPs and \ref{mixDPPs} will immediately generalise those results to DPPs without much effort.

\begin{prop}[Existence of elementary DPPs]\label{eeDPPs}
Let \(K=\sum_{k=1}^m v_kv_k^T\) for some orthonormal set \(V = \left\{ v_k\right\}_{k=1, \dots, m}\subseteq\mathbb R^{\mathcal Y}\). Further, define the measure on \(2^\mathcal Y\) through
\begin{equation}\label{elemDPPs}
\mathbb P(A)\coloneqq \begin{cases} \; \det(K_A) \quad & \text{if } \left\lvert A \right\rvert = m \\ \; 0 & \text{else} \end{cases}.
\end{equation}
Then \(\mathbb P\) is a DPP on \(\mathcal Y\) with marginal kernel \(K\). In particular elementary DPPs exist.
\end{prop}
\begin{proof}
First we have to show that \eqref{elemDPPs} defines a probability measure. For this let \(B\in\mathbb R^{m\times N}\) be the matrix with rows \(v_k\) for \(k=1, \dots, m\). By definition we have \(K = B^TB\) and hence
\begin{equation*}
\begin{split}
\sum_{\begin{subarray}{c} A\subseteq\mathcal Y \\ \left\lvert A \right\rvert = m \end{subarray}} \det(K_A)  = \sum_{\begin{subarray}{c} A\subseteq\mathcal Y \\ \left\lvert A \right\rvert = m \end{subarray}} \det\left(B_{[m]A}\right)^2 = \det\left( BB^T \right) = \det\left( v_{k}^Tv_{l} \right)_{1\le k, l\le m} = 1
\end{split}
\end{equation*}
where we have used the Cauchy-Binet identity and the fact that \(V\) is orthonormal. It remains to check that all marginal probabilities satisfy
\[\mathbb P(A\subseteq \mathbf Y) = \det(K_A).\]
For \(\left\lvert A \right\rvert\ge m\) this follows immediately, so let \(A = \left\{ i_1, \dots, i_{r}\right\}\) for \(r< m\). Then we obtain the marginal probability of \(A\) through summation over the other \(m-r\) points. Namely we have
\begin{equation*}
\begin{split}
\mathbb P(A\subseteq \mathbf Y) & = \sum_{\begin{subarray}{c} A\subseteq J\subseteq[n] \\ \left\lvert J \right\rvert = m \end{subarray}} \mathbb P(J\subseteq\mathbf Y) = \sum_{\begin{subarray}{c} A\subseteq J\subseteq[n] \\ \left\lvert J \right\rvert = m \end{subarray}} \det\left(B_{[m]J}\right)^2 = \det\left( (B^TB)_A \right) = \det(K_A) 
\end{split}
\end{equation*}
where we used Proposition \ref{CB-type}.
\end{proof}

\begin{cor}[Existence of DPPs]
Let \(K\) be a symmetric \(N\times N\) matrix. Then \(K\) is the marginal kernel of a DPP if and only if \(0\le K\le I\).
\end{cor}

Now we can turn towards the simulation of elementary DPPs where we will make use of the previous result. In order to present the algorithms in a compact form we will usually present them in pseudocode. Here, the symbol \(\gets\) stands for the assignment of a value, i.e. \(x\gets y\) means that the variable \(x\) should have the value \(y\) from now on.

\begin{algorithm}
\caption{Sampling from an elementary DPP \label{alg:elementary-DPP-sampling}}
\begin{algorithmic}[1]
\Require{Marginal kernel \(K=\sum_{k=1}^m v_kv_k^T\) for \(\left\{ v_k\right\}_{k=1, \dots, m}\) orthonormal}
\State \(V\gets\left\{ v_k\right\}_{k=1, \dots, m}\)
\State \(Y\gets\varnothing\)
\While{\(\left\lvert V \right\rvert>0\)}
  \State \(p_i\gets Pe_i\) the projection of \(e_i\) onto \(\operatorname{span}(V)\) for \(i\in\mathcal Y\)
  \State Select \(i\in\mathcal Y\) with probability \(\frac1{\left\lvert V \right\rvert} \cdot \left\lVert p_i \right\rVert^2\)
  \State \(Y\gets Y\cup\left\{ i\right\}\)
  \State \(V\gets V_\perp\) an orthonormal basis of the subspace of \(V\) orthogonal to \(p_i\)
\EndWhile
  \State \Return{\(Y\)}
\end{algorithmic}
\end{algorithm}

\begin{prop}[Sampling from elementary DPPs]
Let \(K=\sum_{k=1}^m v_kv_k^T\) where \(\left\{ v_k\right\}_{k=1, \dots, m}\) is a set of orthonormal vectors. Then Algorithm \ref{alg:elementary-DPP-sampling} produces a random variable \(\mathbf Y\) with values in \(2^{\mathcal Y}\) which is an elementary DPP with marginal kernel \(K\).
\end{prop}
\begin{proof}
We note that we only have to check that \eqref{elemDPPs} holds and for this we fix \(A\subseteq\mathcal Y\). The output \(\mathbf Y\) has cardinality \(\left\lvert m \right\rvert\) since no element can be selected twice in the while loop and the size of \(V\) decreases by exactly one in each iteration. Hence, it remains to show
\[\mathbb P(A = \mathbf Y ) = \det(K_A) \]
whenever \(\left\lvert A \right\rvert = m\). Let for the sake of convenience \(A = \left\{ 1, \dots, m\right\}\) and \(\mathcal Y = \left\{ 1, \dots, N\right\}\). It suffices to show that the while loop selects \(1, \dots, m\) in this exact order with probability \(\frac1{m!}\det(K_A)\).

Let \(V_k\) denote the orthonormal set \(V\) in the \(k\)-th step of the while loop and let \(P_{k-1}\) be the projection onto \(\operatorname{span}(V_k)\) and set \(b_i\coloneqq P_0e_i\) for \(i = 1, \dots, N\). We note that if \(1, \dots, k-1\) were selected in the first steps, then \(P_{k-1}\) is exactly the projection to the subspace of \(\operatorname{span}(V_{k-1})\) that is orthogonal to \(b_1, \dots, b_{k-1}\). Since the spaces \(\operatorname{span}(V_k)\) are decreasing we have \(P_kP_j = P_k\) for \(k\ge j\) and thus \(P_{k-1} e_k = P_{k-1}P_0e_k = P_{k-1}b_k \).

 Suppose now that we have selected \(1, \dots, k-1\) in the first \(k-1\) steps of the while loop. The probability to select \(k\) in the next iteration is
\[ \frac1{\left\lvert V_k \right\rvert} \cdot\left\lVert P_{k-1}e_k \right\rVert^2 = \frac1{m-k} \cdot\left\lVert P_{k-1}b_k \right\rVert^2. \]
Thus, the probability to sample \(1, \dots, m\) in this order is equal to
\[\frac1{m!} \cdot\left\lVert b_1 \right\rVert^2\cdot\ldots\cdot\left\lVert P_{m-1}b_m \right\rVert^2. \]
Since \(P_{k-1}\) is the projection onto the subspace orthogonal to \(b_1, \dots, b_{k-1}\), the product is equal to the squared \(m\)-dimensional surface measure of the parallelepiped spanned by \(b_1, \dots, b_m\). It is well known from measure and integration theory that the squared surface is given by the determinant of the Gram matrix
\[\det\begin{pmatrix}
b_1^Tb_1 & \cdots & b_1^Tb_m \\ \vdots & \ddots & \vdots \\ b_m^Tb_1 & \cdots & b_m^Tb_m
\end{pmatrix} = \det\!\big((B^TB)_A\big)\]
where \(B\in\mathbb R^{N\times N}\) is the matrix which columns are equal to \(b_k\). Therefore, it remains to show \(B^TB = K\). However, by definition \(B\) is the projection onto the span of \(\left\{ v_k\right\}_{k=1, \dots, m}\) and thus \(B=K\). Because \(K\) is symmetric like every projection, we have \(B^T = B\) and hence we can conclude \(B^TB = B^2 = B = K\) where we used that \(B\) is a projection.
\end{proof}

\begin{cor}[Cardinality of DPPs]\label{carDPP}
Let \(\mathbb P\) be a DPP with kernel \[K = \sum_{k=1}^N \lambda_kv_kv_k^T.\] Then the cardinality of the DPP is distributed like the sum of the Bernoulli variables \(\left\{ \xi_k\right\}_{k =1, \dots, N}\) with expectations \(\left\{ \lambda_k\right\}_{k=1, \dots, m}\).
\end{cor}
\begin{proof}
To prove this, we only have to convince ourselves that after the Bernoulli experiments the cardinality of a DPP with kernel \eqref{BerKer} has size \(m\coloneqq\sum_{k=1}^N\xi_k\) almost surely. However, this is obvious from the construction of elementary DPPs in Proposition \ref{eeDPPs}.
\end{proof}

Now we can apply Theorem \ref{mixDPPs} to extend the sampling algorithm to general DPPs. 

\begin{algorithm}
\caption{Sampling from a DPP \label{alg:DPP-sampling}}
\begin{algorithmic}[1]
\Require{Eigendecomposition \(\left\{ v_k, \lambda_k\right\}_{k=1, \dots, N }\) of \(K\)}
\State \(J \gets \varnothing\) 
\For{\(k =1, \dots, N\)}
  \State\(J\gets J\cup \left\{ k\right\}\) with probability \(\lambda_k\)
\EndFor
\State \(V\gets\left\{ v_k\right\}_{k\in J}\)
\State \(Y\gets\varnothing\)
\While{\(\left\lvert V \right\rvert>0\)}
  \State \(p_i\gets Pe_i\) the projection of \(e_i\) onto \(\operatorname{span}(V)\) for \(i\in\mathcal Y\)
  \State Select \(i\in\mathcal Y\) with probability \(\frac1{\left\lvert V \right\rvert} \cdot \left\lVert p_i \right\rVert^2\)
  \State \(Y\gets Y\cup\left\{ i\right\}\)
  \State \(V\gets V_\perp\) an orthonormal basis of the subspace of \(V\) perpendicular to \(p_i\)
\EndWhile
  \State \Return{\(Y\)}
\end{algorithmic}
\end{algorithm}
\enlargethispage{.3cm}
\begin{theo}[Sampling algorithm]
Let \(K\in\mathbb R^{N\times N}\) be any symmetric and positive semi-definite matrix such that \(K\le I\). Then the distribution of the output \(\mathbf Y\) of Algorithm \ref{alg:DPP-sampling} is a DPP with marginal kernel \(K\). 
\end{theo}
\begin{proof}
Theorem \ref{mixDPPs} states that an arbitrary DPP is the mixture of elementary DPPs and the for loop in the algorithm represents exactly this mixing with the respective weights. Further, the sampling result for elementary DPPs yields that the output of the second part of the algorithm, namely the while loop, is distributed according to a DPP with marginal kernel \(\sum_{v\in V} vv^T \).
\end{proof}

\begin{rem}
Later on it will usually be more convenient to model or estimate the elementary kernel \(L\) instead of the marginal kernel \(K\). Thus, we should explain how the sampling algorithm would work in this case. Since the two kernels are related by
\[ K = L(L+I)^{-1} \]
their eigendecompositions are closely related. Namely, if \(v\) is an eigenvector of \(L\) with eigenvalue \(\lambda\ge0\), then \(v\) is also an eigenvector of \(K\) with eigenvalue \(\frac{\lambda}{\lambda+1}>0\). After this transformation of the eigendecomposition of \(L\) the sampling algorithm for \(K\) can be applied.
\end{rem}

We close this paragraph with the proof of \ref{mixDPPs} given in \cite{kulesza2012determinantal}.

\begin{proof}[Proof of Theorem \ref{mixDPPs}]
Let \(A\subseteq\mathcal Y, m\coloneqq\left\lvert A \right\rvert\) and set \(W_k\coloneqq (v_kv_k^T)_A\) and \(W_J\coloneqq\sum_{k\in J} W_k\). Then we have
\[\tilde{\mathbb P}(A\subseteq\mathbf Y) = \sum_{J\subseteq[N]} \det(W_J) \cdot\tilde{\mathbb P}\big(\xi_{j} = 1 \text{ if and only if } j \in J\big).\]
Let \(\big((W_{k_1})_1(W_{k_2})_2\cdots (W_{k_m})_m\big)\) denote the \(m\times m\) matrix with \(j\)-th row equal to the \(j\)-th row of \(W_{k_j}\). Using the multilinearity of the determinant we obtain that the marginal probability above is equal to
\begin{equation*}
\begin{split}
& \sum_{J\subseteq[N]}\sum_{k_1, \dots, k_m\in J} \det\big((W_{k_1})_1(W_{k_2})_2\cdots (W_{k_m})_m\big) \cdot\tilde{\mathbb P}\big(\xi_{j} = 1 \text{ if and only if } j \in J\big) \\
 = &\; \sum_{k_1, \dots, k_m=1}^N \det\big((W_{k_1})_1(W_{k_2})_2\cdots (W_{k_m})_m\big) \sum_{J\supseteq\left\{ k_1, \dots, k_m\right\}} \tilde{\mathbb P}\big(\xi_{j} = 1 \text{ if and only if } j \in J\big) \\
 = &\; \sum_{k_1, \dots, k_m=1}^N \det\big((W_{k_1})_1(W_{k_2})_2\cdots (W_{k_m})_m\big) \cdot\tilde{\mathbb P}\big(\xi_{k_j} = 1 \text{ if and only if } j = 1, \dots, m\big) \\
 = &\; \sum_{k_1, \dots, k_m=1}^N \det\big((\lambda_{k_1}W_{k_1})_1(\lambda_{k_2}W_{k_2})_2\cdots (\lambda_{k_m}W_{k_m})_m\big) \\
= & \; \det\bigg(\sum_{k=1}^N \lambda_kW_k\bigg) = \det(K_A).
\end{split}
\end{equation*}
This computation shows that \(\tilde{\mathbb P}\) is a DPP with marginal kernel \(K\).
\end{proof}

\subsubsection{The dual representation}

We will shortly discuss one method how the simulation of DPPs can be made more efficient. The step in the sampling algorithm that takes the longest in practice is the computation of the eigendecomposition of the matrix \(K\) or \(L\). Hence, we will quickly show how this can be reduced to the computation of the eigendecomposition of a smaller matrix.

Consider the matrix \(A = B^TB\in\mathbb R^{N\times N}_{\text{sym}, +}, B\in\mathbb R^{D\times N}\) and set \(C\coloneqq BB^T\in\mathbb R^{D\times D}_{\text{sym}, +}\). Then the spectral decomposition of \(A\) and \(C\) can be related in the following way.
\begin{enumerate}
\item The eigenvalues of \(A\) and \(C\) agree. In fact, if \(v\in\mathbb R^D\) is an eigenvector to the eigenvalue \(\lambda\in\mathbb R\), then \(B^Tv\) is an eigenvector to the eigenvalue \(\lambda\), since
\[AB^Tv = B^TBB^Tv = B^TCv = \lambda B^Tv.\]
\item If \(v\) is a normed eigenvector to the eigenvalue \(\lambda>0\), then we have
\[\big\lVert B^Tv \big\rVert^2 = (B^Tv)^T(B^Tv) = v^TBB^Tv = v^TC^Tv = \lambda \]
and hence \(\frac{B^Tv}{\sqrt{\lambda}}\) is a normed eigenvector to the eigenvalue \(\lambda>0\).
\item Finally, if \(\left\{ v_1, \dots, v_m\right\}\) is an orthonormal set of eigenvectors to the non trivial eigenvalues \(\left\{ \lambda_1, \dots, \lambda_m\right\}\) of \(C\), then
\[\left\{ \frac{B^Tv_k}{\sqrt{\lambda_k}} \;\Big\lvert\; k = 1, \dots, m\right\}\]
is an orthonormal set of eigenvectors to the non trivial eigenvalues of \(A\).
\end{enumerate}

Hence, if \(K\) or \(L\) are given as a gram matrix \(B^TB\), it suffices to compute the eigendecomposition of \(BB^T\in\mathbb R^{D\times D}_{\text{sym}, +}\), which could be significantly faster if \(D<N\). Since the sampling algorithm relies only on the eigendecomposition it can be performed based on the dual representation presented above. It should be mentioned that typically \(L\) will modelled as a gram matrix via the quality diversity decomposition and hence this dual representation will mostly be used for \(L\). Further, it comes with even greater benefits here, since the normalisation constant 
\[\det(L + I) = \prod_{k=1}^N (1 + \lambda_k) = \det\left(BB^T + I\right) \]
reduces to the computation of the determinant of a \(D\times D\) matrix.

The dual representation can make the computations involved with DPPs efficient, but in some cases it might not be effective enough. Therefore, different techniques have been proposed in order to achieve faster computation times, like random projections. This approach relies on the result from \cite{magen2008near} that points in an \(N\)-dimensional space can be randomly projected into a space of dimension \(O(\log(N))\) in such a way, that the volume spanned by those points is almost preserved with a high probability. For a discussion of this approach we refer to \cite{kulesza2012learning}.

\section{Simulation of toy examples}

We will present two examples and although -- or maybe even because -- they are very simple they show how the choice of different parameters in the modelling process affect the DPP. 

\subsubsection{Points on a line}

We start by modelling a one dimensional DPP and simulating from it. More precisely we consider the ground set \(\mathcal Y \coloneqq \left\{ 1, \dots, 100\right\}\). Further, we model the diversity feature vectors like in \ref{moddiv} using reference points and choose \(\mathcal R\coloneqq\mathcal Y\) as a reference set. Now let \(f\) to be a normal density with mean \(0\), i.e. we have
\[(\phi_i)_j\propto \exp\left( - \frac{(i-j)^2}{\sigma}\right) \quad \text{for } i, j\in\mathcal Y.\]
We will choose \(\sigma = 20\) first and then \(\sigma = 5\) to see how this parameter affects the repulsion of the DPP.
Finally we set the qualities to be constant and scale them so that the expected cardinality of the DPP is approximately \(15\). Further, we define a Poisson point process with the same expected cardinality. This means the Poisson point process includes every point independently with probability \(\frac{15}{100}\). A comparison of a sample from those three point processes is depicted in Figure \ref{pointsonaline} and the -- in this case spatially -- repulsive structure of the DPP is apparent.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.60\textwidth]{figures/comparison-different-DPPs-Poisson-7}
	\caption{Two DPPs with a different strength of repulsion on the left and a Poisson point process on a (discretised) line with same expected cardinality. The spatial repulsion of the DPPs is clearly visible.}
	\label{pointsonaline}
\end{figure}

We shall quickly discuss what influence the choice of \(f\) has on the strength of the repulsion of the DPP. For this we let the parameter \(\sigma\) tend to infinity and note
\[\phi_i \xlongrightarrow{\sigma\to\infty} \frac1{\sqrt{N}}\cdot \begin{pmatrix}
1\\ \vdots \\ 1
\end{pmatrix}\in\mathbb R^N. \]
Hence the similarity \(\phi_i^T\phi_j\) between item \(i\) and \(j\) will increase and therefore the negative correlation gets stronger as \(\sigma\) grows. Hence, the DPP will become increasingly repulsive if we increase the parameter \(\sigma\) which can be seen in Figure \ref{pointsonaline} where the first sample corresponds to the choice \(\sigma = 20\), the second one to \(\sigma = 5\). On a more formal level one can argue that if one models the diversity feature vectors this way, we center Gaussian densities at the reference points and then associate an item \(i\) with how likely it is under this Gaussian densities. If we increase the standard deviation \(\sigma\) of this density all items become increasingly similar in relation to the reference points. Similar considerations apply if \(f\) is not a normal density.

\subsubsection*{Binary sequences}

It is well known that a \(0-1\) sequence that is generated by a human and made to look random will typically differ strongly from a randomly generated \(0-1\) sequence.\footnote{At least if the human is sufficiently unfamiliar with statistics.} For example the total amount of changes between zeros and ones is typically significantly higher in the human pseudorandom sequence. Also the length of the longest chain of zeros or ones will likely be significantly shorter (cf. \cite{ruschendorf2014mathematische}). Hence, the position of the ones tend to repell themselves since a human will typically think that a long chain of successive ones will be atypical for a random sequence and thus one could model these positions through a DPP.

We will consider \(0-1\) sequences of length 30 and therefore set \(\mathcal Y \coloneqq \left\{ 1, \dots, 30\right\}\) and define the DPP in the exact same way as above. This means, we choose \(f\) to be a normal density, but will choose the variance such that the repulsion is visible but not too strong and scale the qualities such that the expected cardinality is \(15\), since a human would probably aim to write down around \(15\) ones. In completely analogue fashion to the previous examples we define a Poisson point process with the same expected cardinality. This time we will represent the samples from the two point processes through a \(0-1\) sequence where a \(1\) at the \(i\)-th position indicates that item \(i\) was in the sample. We obtain the two following samples:
\begin{equation*}
\begin{split}
& 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 0 1 1 \\
& 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 0 1 1
\end{split}
\end{equation*}
Although the first sequence might actually look more random at first, this is the one generated by the DPP and on second sight one realises that the positions of the ones are rather spread out, i.e. negatively correlated. Indeed in the first sequence the longest chain of zeros or ones is of length three, in the second one of length seven. The amount of changes between zero and one is \(22\) in the first sequence and only \(14\) in the second sequence.

Although the DPP presented above incorporates some of the properties one might expect from a \(0-1\) sequence created by a human, this process will not be exactly determinantal. However, it shall be noted that different \(0-1\) sequences studied in probability theory exhibit an exact determinantal structure. For example Borodin-Diaconis-Fulman studied the sequence of descent positions in \cite{borodin2010adding}. To obtain those sequences one first samples a sequence of \(N+1\) independent digits \(\left\{ 0, 1, \dots, 9\right\}\). Then one marks the positions in \(\left\{ 1, \dots, N\right\}\) where the successor of the digit is strictly smaller than the current digit. The heuristical argument why those positions of descent repell themselves is that if \(k\) is not a point of descent, then the digit on the position \(k+1\) is likely to be big and hence likely to be a point of descent.

\subsubsection{Points in a square}

This time we want to built a DPP in a two dimensional square \([0,1]^2\) or at least a discrete approximation of it. This might be used to model the positions of trees that repel themselves due to a competition for natural resources, positions of football players on the field or the positions people choose for a picnic in a park.

In order to do this we follow an approach similar to the case of the one dimensional DPPs. Hence, we set
\[\mathcal Y \coloneqq 99^{-1} \left\{ 0, \dots, 99\right\}^2 \]
and obtain a \(100\times 100\) grid covering the unit square. Again, we choose \(\mathcal R \coloneqq \mathcal Y \) and \(f\) to be the normal density with mean \(0\) and variance \(\sigma>0\). Then we choose the similarity feature vectors to be 
\[(\phi_i)_j\propto f(\left\lVert i - j \right\rVert) \quad \text{for } i, j\in\mathcal Y.\]
where \(\left\lVert \cdot \right\rVert\) is the Euclidean norm. We propose constant qualities just like before and scale them and also the variance \(\sigma\) in such a way that we get a reasonable cardinality and also a notable repulsion of the DPP. 
The resulting samples are depicted in Figure \ref{pointsinthesquare}.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.444\textwidth]{figures/DPP-in-square-large-3}
	\includegraphics[width=0.44\textwidth]{figures/Poisson-in-square-large-2}
	\caption{A DPP (right) and a Poisson point process (left) on a \(100\times100\) grid in the unit square with the same expected cardinality. The -- in this case spatially -- repellent structure of the DPP is clearly visible.}
	\label{pointsinthesquare}
\end{figure}

It should be noted at this stage, that the simulation of the DPP can still be performed in relatively short time, even without making use the dual sampling or the random projections. In fact, one sample could be produced on a \(1.8\si{GHz}\) Intel Core i5 with \(8\si{GB}\) RAM\footnote{I used a MacBook Air from 2012.} in about two minutes. This is actually quite astonishing given that the DPP is a discrete probability distribution over \(2^{10^4} \approx 10^{3000}\) elements. This is roughly the estimated number of elementary particles in the universe to the power of 35.

Assume now that we have some reason to believe that the qualities of the individual points on the grid are not equal. Maybe there might be a road just around the park and hence people prefer to sit in the middle of the park and we can implement this into our model by letting the qualities of the items decrease depending on their distance to the centre \(m \coloneqq(0.5, 0.5)\) of the grid. More precisely we choose
\[q_i\coloneqq a \cdot \exp\big(-b \left\lVert i - m \right\rVert\big) \]
where \(a, b>0\) are scaled such that the decrease of quality is visible but not too strong and that a reasonable cardinality of the DPP is obtained. The results for this can be seen in Figure \ref{pointsinthesquarelog}.
\begin{figure}[h!]
	\centering
	\includegraphics[width=0.60\textwidth]{figures/DPP-in-square-large-log-linear}
	\caption{A DPP on a \(100\times100\) grid in the unit square with decreasing qualities towards the edges.}
	\label{pointsinthesquarelog}
\end{figure}

\clearpage