\chapter{Learning setups}

\section{What does learning mean?}

\section{Motivation for learning DPPs}

\section{Direct reconstruction of the kernel}

In this section we want to see how we can learn, i.e. how we can estimate the marginal kernel from observations \(\mathbf Y_1, \dots, \mathbf Y_n\) that are distributed according to \(\mathbb P\) and will sketch the procedure in \cite{urschel2017learning}. Let \(\hat{\mathbb P}_n\) be the empirical distribution
\[\hat{\mathbb P}_n \coloneqq \frac1n\sum_{i=1}^n\delta_{\mathbf Y_i}.\]
It is well known that due to the strong law of large numbers the empirical distributions weakly converge to \(\mathbb P\) if the observations are independent. Therefore we can consistently estimate all principle minors of \(K\), since
\[\hat{\mathbb P}_n(A\subseteq \mathbf Y) \xlongrightarrow{n\to\infty} \mathbb P(A\subseteq\mathbf Y) = \det(K_A) \quad \text{almost surely}.\]

Thus the question naturally arrises whether we can reconstruct the kernel \(K\) from the knowledge of all of its principle minors. This is known as the \emph{principle minor assignment problem} and has been studied extensively (cf. \cite{griffin2006principal} and \cite{urschel2017learning}) and an computationally efficient algorithm has been proposed for the problem in \cite{rising2015efficient}. It is in fact possible to retain the matrix from its principle minors up to an equivalence relation which identifies matrices with each other, that have the same principle minors. Obviously this is sufficent for the task of learning a DPP, because those matrices are exactly those who give rise to the same point process. To see roughly how this reconstruction works we note that the diagonal is given by
\[K_{ii} = \det(K_{\left\{ i\right\}})\]
and the absolute value of the off diagonal can be obtained through
\[K_{ij}^2 = K_{ii} K_{ii} - \det\big(K_{\left\{ i,j\right\}}\big). \]
The reconstruction of the signs of the entries \(K_{ij}\) turns out to be the main problem, but this can be done analysing the cycles of the adjacency graph \(G_K\) corresponding to \(K\). The adjacency graph has \(\mathcal Y\) as its vertex set and the set of edges consists of the pairs \(\left\{ i,j\right\}\) such that \(K_{ij}\ne0\). The reconstruction now relies on the analysis of the cycles of this graph and it has been shown, that one only needs to know all the principle minors up to the order of the cycle sparsity of \(G_K\) (cf. \cite{urschel2017learning}). Following this method it is possible to compute estimators \(\hat K_n\) of \(K\) in polynomial time and give a bound on the speed of convergence in some suitable metric.

In completely analogue fashion one can learn the elementary kernel \(L\) and those estimations can be used to sample from a DPP that was observed. This procedure might be sufficient in some scenarios, but this approach lacks the ability to extrapolate the knowledge one has of specific DPPs onto some new, unobserved DPPs which is exactly the point that would distinguish the procedure from classical statistics and would allow for far more interesting applications. To achieve this, we introduce the notion of conditional DPPs in the following section which are customised to describe families of DPPs with kernels that are in some way similar.

\section{A learning approach to estimating the kernel}

The approach described above is clearly of traditional statistical type and we want to touch on how the kernel estimation can be put into a machine learning task (cf. \cite{affandi2014learning}). For this we assume that we have a training set, i.e. a number of subsets \(Y_1, \dots, Y_T\subseteq\mathcal Y\) drawn independently and according to the DPP \(\mathbb P\). We aim to establish a quantity that gives an intuitive measure of how well a different DPP describes the training set and then we want to find the elementary kernel \(L\) for which the associated DPP \(\mathbb P_L\) optimises this quantity. A widely used choice in the machine learning community for this is the so called \emph{log likelihood function}
\[\log\left(\prod_{t = 1}^T \mathbb P_L(Y_t)\right)\]
and we will work with its negative
\[\mathcal L(L) \coloneqq -\log\left(\prod_{t = 1}^T \mathbb P_L(Y_t)\right) = -\sum\limits_{t=1}^T\log\left( \det(L_{Y_t})\right) + T \log\left( \det(L+I)\right)\]
where high values of \(\mathcal L\) correspond to kernels \(L\) where at least one element \(Y_t\) of our training set is very unlikely. Thus it is natural to minimise the loss function \(\mathcal L\) over all positive semidefinit \(L\in\mathbb R^{N\times N}\). Note that we have \(\mathcal L(L) = \infty\) if and only if an observation \(Y_t\) of our training set is impossible under the DPP \(\mathbb P_L\), i.e. we do not consider those kernels in our estimation. We note that the loss function is smooth and the gradient of this can be explicitly expressed, at least on the domain \(\left\{ \mathcal L<\infty\right\}\). This is due to the fact that the determinants of the submatrices are polynomials in the entries of \(L\) and the composition of those with the smooth function \(\log\colon(0,\infty)\to\mathbb R\) stays smooth. This property allows the use of gradient methods but they face the problem that the loss function is non convex and thus those algorithms will generally not converge to a global minimiser (cf. \cite{affandi2014learning}). Nevertheless it is worth investigating whether those local minima turn out to produce good results in real world scenarios.

\subsection*{Learning kernels of conditional DPPs}

\section{Learning the quality functions}

\section{Estimating the mixture coefficients of \(k\)-DPPs}