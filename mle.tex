\section{Maximum likelihood estimation}

The method of maximum likelihood estimation (MLE) is a very well established procedure to estimate parameters. The philosophy of MLE is that one selects the parameter under which the given data would be the most likely to be observed and in order to present the general procedure we follow the corresponding section in \cite{rice2006mathematical}.

Suppose we have given some candidates  \(f(x_1, \dots, x_n| \theta)\) for the joint density of some random variables \(X_1, \dots, X_n\) with respect to some reference measure \(\prod_{i=1}^n\mu(\mathrm d x_i)\) and we want to decide which parameter \(\theta\in\Theta\) describes the realisations \(x_1, \dots, x_n\), which we will also call data or observation, best. Hence, it is reasonable to pick \(\theta\) under which the observations \(x_1, \dots, x_n\) are the most likely. In other words we want to find the parameter \(\theta\) that maximises the density \(f(x_1, \dots, x_n| \theta)\). If additionally the random variables are indepent and identically distributed, their joint density factorises and thus we obtain
\[f(x_1, \dots, x_n| \theta) = \prod_{i=1}^n f(x_i| \theta) \]
where \(f(x| \theta)\) is the density with respect to \(\mu\) of the \(X_i\). In practice it is often easier to maximise the logarithm of the density
\[\mathcal L(\theta) = \log\!\big(f(x_1, \dots, x_n| \theta)\big) = \sum_{i=1}^n \log\!\big(f(x_i| \theta)\big) \]
 since this transforms the product over functions into a sum. However, this is clearly equivalent to maximising the density since the logarithm is strictly monotone.

\begin{defi}[Maximum likelihood estimator]
Let \(\Theta\) be a set, which we call the \emph{parameter set} and let
\[\mathcal F = \Big\{ f(\cdot| \theta) \colon\mathcal X\to [0, \infty) \;\big\lvert\; \theta\in\Theta \Big\}\]
be a family of probability densities with respect to some measure \(\mu\) on some measurable space \(\mathcal X\). We call the function
\[\mathcal L\colon\Theta\to[-\infty, 0], \quad \mathcal L(\theta)\coloneqq \sum_{i=1}^n\log(f(x_i\mid \theta)) \]
the \emph{log likelihood function} associated with the observations \(x_1, \dots, x_n\) and its maximiser
 \begin{equation}\label{mledef}
 \hat \theta_n\coloneqq \underset{\theta\in\Theta}{\arg\max} \mathcal L(\theta)
 \end{equation}
the \emph{maximum likelihood estimator} or short \emph{MLE}.
\end{defi}

\subsubsection*{Very short reminder on optimisation}

Since the calculation of the MLE is a maximisation task, it is suitable to review some general properties of optimisation problems. For this let \(U\subseteq\mathbb R^M\) and \(f\colon U\to \mathbb R\) be a function. In practice the maximisation
\[\hat x\coloneqq \underset{x\in U}{\arg\max} f(x) \]
will usually not be explicitly solvable and therefore one has to exploit numerical algorithms. 
 
Those work particularly well if the function \(f\) is concave and possibly smooth and one powerful method is given by the so called gradient descent. To quickly explain its philosophy, we note that \(\nabla f\) points into the direction of the steepest ascent of the function \(f\) and thus an intuitive approach the maximise \(f\) would be to follow the gradient, i.e. to take a solution \(\gamma\) of the gradient flow \(\gamma^\prime = \nabla f(\gamma)\) and work out its limit. However, if the function is not concave one can not even guarantee that the gradient flow reaches a local minimum, since one can construct examples where \(\gamma\) gets stuck in a critical point. However, in the concave case this suffices since critical points and global minima agree for concave functions. The gradient descent is an algorithm derived from this observation and is essentially a discretisations of the gradient flow meaning that it iteratively takes small steps into the direction of the gradient and thus lowers the value of the function. Some more sophisticated versions of gradient descent methods even consider higher order derivatives and use the information they provide over the geometry of the graph. Generally speaking those algorithms work extremely well even in high dimensions and thus their efficiency and stability have been studied broadly and we refer to the extensive monograph \cite{boyd2004convex}. All together we note that concavity is an extremely favourable property for a function that shall be maximised, which will be the log likelihood function later on.
 
A second property which is important in the existence theory of maximisers is the \emph{coercivity} of the function in the sense that
\[f(x)\to-\infty \quad \text{for } \left\lvert x \right\rvert\to\infty.\]
In fact every (upper semi-) continuous and coercive function defined on a closed set \(U\subseteq\mathbb R^M\) attains its minimum. To see this one can fix \(x_0\in U\) and use the coercivity to obtain \(f<f(x_0)\) outside of a compact set \(K\) and thus the supremum of \(f\) agrees with the supremum of \(f\) over \(K\) which it is attained. We will later introduce some abstract theory about the consistency of estimators and for this we will need this result in a more general setting. However, the version above is enough in the case of the maximum likelihood estimators for parameters of DPPs and therefore readers that are not familiar with elementary notions of topology are advised to neglect the following statement.

\begin{prop}[Existence of maximisers]\label{dirmet}
Let \(\mathcal X\) be a topological Hausdorff space and \(f\colon \mathcal X\to [-\infty, \infty)\) be an \emph{upper semicontinuous} function, i.e.
\[L_f(\alpha)\coloneqq\left\{ x\in\mathcal X\mid f(x) \ge \alpha\right\}\]
is closed for all \(\alpha\in\mathbb R\). Further, we will assume that \(f\) is \emph{coercive}, meaning that for any \(\alpha\in\mathbb R\) the set \(L_f(\alpha)\) is compact. Then \(f\) attains its maximum in at least one point, i.e. there is \(\hat x\in\mathcal X\) such that
\[f(\hat x) = \sup_{x\in\mathcal X} f(x). \]
\end{prop}
\begin{proof}
Let without loss of generality \(f\) be not identical to \(-\infty\) because otherwise the statement is trivial. Then we have
\[\alpha\coloneqq\sup_{x\in\mathcal X} f(x) > - \infty.\]
If we choose \((\alpha_n)\) to be strictly increasing towards \(\alpha\), then we get \(L_f(\alpha_{n+1}) \subseteq L_f(\alpha_n)\) for all \(n\in\mathbb N\) and further none of the sets \(L_f(\alpha_n)\) is empty. By the Cantor intersection theorem\footnote{A precise formulation can be found in the appendix.} we get that also the intersection is non empty, i.e there is
\[\hat x\in \bigcap_{n\in\mathbb N} L_f(\alpha_n).\]
This implies
\[f(\hat x) \ge \alpha_n\xlongrightarrow{n\to\infty} \alpha = \sup_{x\in\mathcal X} f(x).\]
\end{proof}


\subsection{Presentation of different models}\label{models}

Let in the following \((\mathbf Y_n)_{n\in\mathbb N}\) be a sequence of independent random variables distributed according to a DPP. In order to follow the MLE approach, we need to express the density of the point process with respect to some measure and we will do this by giving the elementary probabilities which are nothing but the densities with respect to the counting measure. Thus, we will assume that we are dealing with \(L\)-ensembles in this section. Since the observations \((\mathbf Y_n)_{n\in\mathbb N}\) are defined on some common probability space which we will denote by \((\Omega, \mathbb P)\), we will write
\[f(A| \theta) = \frac{\det(L(\theta)_A)}{\det(L(\theta) + I)} \]
for the elementary probabilities of the DPP that arises from the parameter \(\theta\). We will now present the maximum likelihood estimators for different parametric classes. 

\subsubsection*{MLE of the elementary kernel \(L\)}

The most intuitive parameter that one can estimate is the elementary kernel \(L\) itself since it parametrises the entire class of \(L\)-ensembles.

\begin{emp}[Maximum likelihood estimator for \(L\)]
We consider the parameter space \(\Theta = \mathbb R^{N\times N}_{\text{sym}, +}\) of positive semi-definite symmetric matrices and the parametric family 
\[\mathcal F = \Big\{ f(\cdot| L) \;\big\lvert\; L\in \mathbb R^{N\times N}_{\text{sym}, +}\Big\}\]
where \(f(A| L)\propto \det(L_A)\) is the elementary probability of DPP with elementary kernel \(L\). We seek to find the MLE 
\[\hat L_n \coloneqq \underset{L\in\mathbb R^{N\times N}_{\text{sym}, +}}{\arg\max}\, \mathcal L(L). \]
The log likelihood function is now given by
\[\mathcal L \colon\mathbb R^{N\times N}_{\text{sym}, +} \to [-\infty, 0], \qquad L \mapsto \log\left(\prod_{i = 1}^n f(\mathbf Y_i| L)\right).\]
\end{emp}
Using \eqref{e2.3} we get the expression
\begin{equation}\label{e3.1}
\mathcal L(L) = \sum\limits_{i=1}^n\log\left( \det(L_{\mathbf Y_i})\right) - n \log\left( \det(L+I)\right)
\end{equation}
which is upper semi-continuous in \(L\). Although the parametric family that arises from the elementary kernels \(L\) parametrises the whole class of \(L\)-ensembles and therefore gives a high variety of different associated \(L\)-ensembles, we will see that it makes the computation of the MLE more complex. Therefore, we introduce smaller classes of \(L\)-ensembles, which will decrease the flexibility of the model, but make computation more efficient.

\subsubsection*{MLE of the qualities}

Unlike earlier we will not try to estimate the whole kernel \(L\) but only the qualities \(q_i\) of the items \(i\in\mathcal Y\). More precisely we recall that we can parametrise the positive definite symmetric matrices \(L\) using the quality diversity parametrisation
\[ (q, \phi) \mapsto \Psi(q, \phi) = L \quad \text{where } L_{ij} = q_i\phi_i^T\phi_jq_j.\]
Now we fix a diversity feature matrix \(\hat \phi\), that we will usually model according to some perceptions we might have and set \(\hat S_{ij}\coloneqq\phi_i^T\phi_j\). Now we aim to estimate the quality vector \(q\in\mathbb R_+^N\) instead of the whole kernel \(L\). This means that we optimise the likelihood function over a smaller set of kernels, namely the ones of the form \(\Psi(q, \hat \phi)\) for \(q\in\mathbb R_+^N\). Obviously the maximal likelihood that can be achieved using this more restrictive model decreases since we consider less positive definite matrices and we have
\[\max_{q\in \mathbb R_+^N} \mathcal L(\Psi(q, \hat \phi)) \le \max_{L\in \mathbb R_{\text{sym}, +}^{N\times N}} \mathcal L(L). \]

\begin{emp}[Maximum likelihood estimator for the quality]
This time we work with the parameter set \(\Theta = \mathbb R_+^N\) and the parametric family
\[\mathcal F = \Big\{ f(\cdot| q) \;\big\lvert\; q\in \mathbb R_+^N\Big\}\]
where \(f(A| q)\propto \det(\Psi(q, \hat \phi)_A)\) is the elementary probability of DPP with elementary kernel \(\Psi(q, \hat \phi)\). 
 We aim to find the MLE of the quality vector \(q\in\mathbb R_+^N\), in other words we set 
\[\hat q_n \coloneqq \underset{q\in\mathbb R_+^N}{\arg\max}\, \mathcal L(q) \]
where we perceive the likelihood function as a function of \(q\).
\end{emp}

Using \eqref{e2.4} we obtain the following expression for the single summands of the log likelihood function 
\begin{equation}\label{loglikqua}
\log\left( \prod_{j\in Y_i} q_j^2\right) + \log(\det(\hat S_{Y_i})) - \log\left(\sum_{A\subseteq \mathcal Y} \prod_{j\in A}q_j^2\det(\hat S_A) \right)
\end{equation}
and note that it is upper semicontinuous.

\subsubsection*{Log linear model for the qualities}

The motivation for restricting our ambitions of estimation to the qualities \(q_i\) rather than the whole elementary kernel \(L\in\mathbb R_{\text{sym}, +}^{N\times N}\) was to obtain a more tractable optimisation problem. Unfortunately we can tell from \eqref{loglikqua} that the log likelihood still isn’t concave in \(q\) and in order to achieve this, we will introduce the following model for the qualities.

\begin{emp}[Log linear model for the qualities and MLE]
From now on we will fix vectors \(f_i\in\mathbb R^M\) for \(i\in\mathcal Y\) and call them \emph{feature vectors}. Further, we set
\[q_i = \exp\left(\theta^T f_i\right)\quad \text{for } \theta\in\mathbb R^M\]
and will only consider quality vectors \(q\in\mathbb R_+^N\) that have this form. To formulate the maximum likelihood estimator for \(\theta\) we set \(\Theta \coloneqq\mathbb R^M\) and consider the parametric family
\[\mathcal F = \left\{ f(\cdot| \theta) \;\big\lvert\; \theta\in\mathbb R^M\right\}\]
where \(f(\cdot| \theta)\) is the density of the DPP with diversity feature matrix \(\hat \phi\) and the according qualities \(q_i = \exp\left(\frac12 \theta^T f_i\right)\). Further, we will consider the maximum likelihood estimator
\[\hat{\theta}_n\coloneqq \underset{\theta\in\mathbb R^M}{\arg\max}\, \mathcal L(\theta) \]
where we regard \(\mathcal L\) again as function of \(\theta\). We will assume that the feature vectors \(f_i\) span the whole space \(\mathbb R^M\) because otherwise we can simple work with the projections of the parameter \(\theta\) onto the span of the feature vectors and obtain an equivalent model.
\end{emp}

\begin{rem}
It shall be noted that although this log linear model seems to be a harsh restriction, it isn’t a restriction at all, at least theoretically. If we take \(M=N\) and choose \(f_i\) to be the unit vectors in \(\mathbb R^N\), then this is just a logarithmic transformation of the parameters and thus the maximal likelihood that can be achieved with this model does not change. In practice it will be of interest to work with rather low dimensional parameters \(\theta\), because if the ground set \(\mathcal Y\) gets large, optimisation in \(\mathbb R^N\) can be inefficient. In this case of course the maximal likelihood under the optimal parameter may decrease, however, the approximation of the optimal parameter might become possible again which justifies this sacrifice.
\end{rem}

Again, we can express the log likelihood function explicitely and note that it is upper semi-continuous. In fact, it takes the form 
\begin{equation}\label{e3.31}
2\cdot\theta^T\sum\limits_{i\in Y}f_{i} + \det(\hat{S}_{Y}) - \log\left( \sum_{A\subseteq\mathcal Y} \exp\left(2\cdot \theta^T\sum_{i\in A}f_{i}\right)\det(\hat{S}_A) \right).
\end{equation}

\subsection{Coercivity and existence of the maximum likelihood estimators}

A priori it is not clear that the maximum likelihood estimators exist and we will actually see that they do not exist in general. However not everything is lost since we will show that the probability that they exist tends to one for increasing sample size. We will also shortly discuss a second method how one can slightly adjust the concept of MLE to obtain the general existence of the estimator which we will do by a regularisation term.

\subsubsection*{MLE of the qualities}

The MLE \(\hat q_n\) does not exist for all realisations \((Y_n)_{n\in\mathbb N}\) of \((\mathbf Y_n)_{n\in\mathbb N}\). To see this, we suppose that we have only one sample \(Y_1 = \mathcal Y\) which is the whole set. The higher the qualities of the items are, the more likely this observation gets and therefore the maximum of the log likelihood function -- which is \(0\) in this case -- is not obtained. This can also be made rigorous in the following computation. For this we assume that \(\det(\hat{S}_{\mathcal Y})>0\) and that the qualities are constant, then the log likelihood function takes the form
\begin{equation*}
\begin{split}
\log\left(q^{2N} \det(\hat S_{\mathcal Y})\right) - \log\left(\sum_{A\subseteq \mathcal Y}q^{2\left\lvert A \right\rvert}\det(\hat S_A) \right) = \log\left( \frac{q^{2N} \det(\hat S_{\mathcal Y})}{\sum_{A\subseteq \mathcal Y}q^{2\left\lvert A \right\rvert}\det(\hat S_A)}\right) \xlongrightarrow{q\to\infty} 0.
\end{split}
\end{equation*}
However this maximum is never attained, since for every \(L\)-ensemble we have \(\mathbb P_L(\varnothing) > 0\) and therefore
\[\mathcal L(q) = \log\left(\mathbb P_{\Psi(q, \hat S)}(\mathcal Y)\right) < 0 \quad \text{for every } q\in\mathbb R_+^N. \]

The thing that goes wrong in this case is, that under the observation of the whole set \(\mathcal Y\) we would estimate a deterministic model that always selects the whole set, namely the DPP with marginal kernel \(I\). Since all of the eigenvalues are \(1\) in this case, this DPP is not a \(L\) ensemble and therefore we can not describe it with the quality diversity decomposition.
However if we assume that the data is actually generated by a \(L\)-ensemble, then such a scenario becomes unlikely as the sample size increases. We will fix this in the following result.

\begin{prop}[Coercivity and existence of the MLE]
Let \(\mathbf Y_1, \mathbf Y_2, \dots\) be a sequence of independent and identically distributed point processes that belong to the class of \(L\)-ensembles. Then we have
\[\mathbb P\left(\hat q_n\in\mathbb R_+^N \text{ exists}\right) \ge \mathbb P\big(\mathcal L \text{ is coercive}\big) \xlongrightarrow{n\to\infty} 1.\]
\end{prop}
\begin{proof}
The first inequality follows from Proposition \ref{dirmet} since the log likelihood function is upper semicontinuous. We will show that \(\mathcal L\) is coercive if one of the observations is the emptyset. Then the claim follows from
\begin{equation*}
\begin{split}
\mathbb P\big(\mathcal L \text{ is coercive}\big) & \ge \mathbb P\left(\bigcup_{i=1}^n \left\{ \mathbf Y_i = \varnothing\right\}\right) = 1 - \mathbb P\left(\bigcap_{i=1}^n \left\{ \mathbf Y_i \ne\varnothing\right\}\right) \\ 
& = 1 - \mathbb P(\mathbf Y_1 \ne\varnothing)^n \xlongrightarrow{n\to\infty} 1
\end{split}
\end{equation*}
since we have \(\mathbb P(\mathbf Y_1 \ne\varnothing) < 1\) for every \(L\)-ensemble.

So let \(Y_1, \dots, Y_n\) be some observations with \(Y_i=\varnothing\) for at least one \(i\in\left\{ 1, \dots, n\right\}\) and let \((q^k)_{k\in\mathbb N}\subseteq\mathbb R_+^N\) be a  sequence such that \(\lvert q^k \rvert\to \infty\). Note that it suffices to show that every subsequence of \((q^k)\) contains a subsubsequence \((q^l)\) such that 
\[\mathcal L(q^l)\to-\infty\quad\text{for } l\to\infty.\]
Hence we fix a subsequence of \((q^k)\) which we denote by \((q^k)\) again in slightly abusive notation. Let \((q^l)\) be a subsequence of \((q^k)\) such that one coordinate diverges to infinity, i.e.
\[q_{j_0}^l\xlongrightarrow{l\to\infty} \infty\quad\text{for one } j_0\in\left\{ 1, \dots, N\right\}.\]
The \(i\)-th summand of \(\mathcal L\) takes the form
\[-\log\left( \sum_{A\subseteq\mathcal Y}\prod_{j\in A}(q^l_j)^2\det(\hat S_A)\right) \le - \log\left( (q^l_{j_0})^2\right)\xlongrightarrow{l\to\infty} -\infty\]
where we used \(\hat S_{\left\{ j_0\right\}} = 1\). Because the other summands are non positive this implies
\[\mathcal L(q^l)\xlongrightarrow{l\to\infty} -\infty\]
which we had to show.
\end{proof}

\begin{rem}
The proof above should be read in the following way. The statement \(q^l_{j_0}\to\infty\) is equivalent to a model that would always select the item \(j_0\). However, since we have observed the empty set, the observations would be impossible under this model and thus the log likelihood function takes the value \(-\infty\) for this model.
\end{rem}

\begin{prop}[Positivity of the MLE]\label{PosMLE}
Assume that \(\mathbf Y_1, \mathbf Y_2, \dots\) is a sequence of independent and identically distributed point processes that are distributed according to a \(L\)-ensemble with strictly positive qualities. Then we have
\[\mathbb P\left(\hat q_n\in\mathbb R_+^N \text{ exists and } \hat q_n\in (0, \infty)^N\right) \xlongrightarrow{n\to\infty} 1.\]
\end{prop}
\begin{proof}
We have already seen that the probability that the MLE exists tends to one, so we only have to show that the probability that the estimated qualities are strictly positive tends to one. The approach to prove this is exactly the same than in the proof of existence. Indeed we note that once \(j\) occurs in one of the observations \(Y_1, \dots, Y_n\) we have \(\mathcal L(q) = -\infty\) for every \(q\in\mathbb R_+^N\) with \(q_j = 0\). Therefore, we have \((\hat q_n)_j>0\) if \(j\in Y_i\) for at least one \(j\in\left\{ 1, \dots, n\right\}\). Finally we note that the probability that \(j\) occurs in the \(i\)-th sample is strictly positive since we have
\[\mathbb P(j\in\mathbf Y_i)\ge\mathbb P(\left\{ j\right\} = \mathbf Y_i) = q_j^2>0. \]
\end{proof}

\subsubsection*{MLE of the elementary kernel}

We can quite easily adapt the proof for the existence of MLEs of the qualities to the case of MLEs for the whole elementary kernel \(L\).

\begin{prop}[Coercivity and existence of MLE]
Let \(\mathbf Y_1, \mathbf Y_2, \dots\) be asequence of independent and identically distributed point processes that fall in the class of \(L\)-ensembles. Then we have
\[\mathbb P\left(\hat L_n\in\mathbb R^{N\times N}_{\text{sym}, +} \text{ exists}\right) \ge \mathbb P\big(\mathcal L \text{ is coercive}\big) \xlongrightarrow{n\to\infty} 1.\]
\end{prop}
\begin{proof}
Again it suffices to show \(\mathcal L(L)\to -\infty\) for \(\left\lvert L \right\rvert\to\infty\) once we have observed the empty set once. To see this, we use the quality diversity parametrisation
\[\Psi\colon\mathbb R_+^N\times\mathbb S_N^N\to\mathbb R^{N\times N}_{\text{sym}, +}, \quad (q, \phi)\mapsto \left( q_i\phi_i^T\phi_j q_j\right)_{1\le i, j\le N}.\]
Note that since \(\Psi\) is continuous and \(\mathbb S_N^N\) is bounded, \(\left\lvert \Psi(q, \phi) \right\rvert\to\infty\) implies \(\left\lvert q \right\rvert\to\infty\). The exact same calculations as in the previous proof show
\[\mathcal L(L) = \mathcal L(\Psi(q, \phi))\to-\infty\quad \text{for }\left\lvert L \right\rvert\to\infty.\]
\end{proof}

\subsubsection*{Coercivity for the log linear model}

We proceed just like before.

\begin{prop}[Coercivity and existence of MLE]
Assume that \(\mathbf Y_1, \mathbf Y_2, \dots\) is a sequence of independent and identically distributed point processes that are distributed according to a \(L\)-ensemble with strictly positive qualities. Then we have
\[\mathbb P\left(\hat \theta_n\in\mathbb R^M \text{ exists}\right) \ge \mathbb P\Big(\mathcal L \text{ is coercive as a function on } U \Big) \xlongrightarrow{n\to\infty} 1.\]
\end{prop}
\begin{proof}
Again we show that \(\mathcal L\) is coercive on \(\mathbb R^M\) whenever we have observed the emptyset as well as every item at least once. Let now \((\theta^k)_{k\in\mathbb N}\subseteq U\) be a sequence such that \(\lvert \theta^k \rvert\to\infty\). Then there is at least one index \(i\in\left\{ 1, \dots, N\right\}\) and a subsequence \((\theta^l)_{l\in\mathbb N}\) such that 
\[f_i^T\theta^l\to\infty\quad \text{or } f_i^T\theta^l\to-\infty \quad \text{for } l\to\infty\]
since otherwise all sequences \(\big(f_i^T\theta^l\big)\)  therefore also \((\theta^l)\) would be bounded. However, this is equivalent to
\[\exp(f_i^T\theta^l)\to\infty\quad \text{or } \exp(f_i^T\theta^l)\to0 \quad \text{for } l\to\infty\]
and we have seen in the proof of \ref{PosMLE} that the log likelihood function tends to \(-\infty\) in this case.
\end{proof}

\subsubsection*{MLE with regularisation or MAP estimation}

We have seen that the probability that the MLE exists tends to one if the sample size goes to infinity. However, it might not be possible in practice to obtain larger data sets and therefore we introduce a variation of maximum likelihood estimation which forces the existence of a maximiser. The idea is to add a coercive function to the log likelihood function such that the sum is coercive and to optimise this sum.

\begin{defi}[MAP estimation]
Let the setting be the same as for the normal maximum likelihood estimation. Further we assume that we have given a function \(R\colon\Theta\to[-\infty, 0]\) which we call the \emph{regulariser}. The \emph{regularised MLE}, \emph{maximum a posteriori probability} or shortly \emph{MAP estimator}\footnote{The term maximum posteriori probability will only properly make sense once we introduce the Bayesian setting in the next chapter. We will see there that the regularised MLE is nothing but the mode of the posterior density.} is the maximiser
\[ \hat \theta_n\coloneqq \underset{\theta\in\Theta}{\arg\max} \big( \mathcal L(\theta) + R(\theta)\big).\]
\end{defi}

\begin{rem}
\begin{enumerate}
\item The regularised maximum likelihood approach is clearly an extension of the classical approach since one can simply set \(R=0\).
\item If the regulariser is upper semi-continuous and coercive, then the MAP estimator for DPPs always exists. In fact, \(\mathcal L\le0\) implies that \(\mathcal L + R\) is coercive and also upper semi-continuous since the log likelihood functions are upper semi-continuous for all parametric models and \ref{dirmet} yields the assertion. 
\item The regularised MLE and the MLE can agree but don’t necessarily agree. 
\item The regulariser can be used to encode any prior conceptions one has. For example one can use the regulariser to change the parameter set. Indeed, if we want to consider all matrices \(\mathbb R^{N\times N}\) as a parameter space instead of only the symmetric positive semi-definite matrices, we can simply set
\[R(L)\coloneqq\begin{cases} \;0 \quad&\text{if } L \text{ symmetric and positive semi-definite} \\ -\infty &\text{otherwise}\end{cases}\]
which is equivalent to the MLE on the smaller parameter space.
The advantage of \(\mathbb R^{N\times N}\) as a parameter space is that one can make use of pre-implemented optimisation algorithms, since they are usually defined over real vector spaces.
\item We will see in the last chapter how different choices of the regulariser affect the estimator.
\end{enumerate}
\end{rem}

\subsection{Consistency of the maximum likelihood estimators}

We will now turns towards the question of consistency of the maximum likelihood estimators introduced earlier in this section. For this we will first give a formal proof of the consistency of the MLE and then present a rather general framework that will allow us to turn the formal proof into a rigorous one.

\begin{emp}[Formal proof of consistency]
We will consider a general MLE like in \eqref{mledef} and we will assume that the observations \((X_n)\) are independent and have density \(f(x| \theta_0)\) with respect to the reference measure \(\mu\). By the law of large number we have
\begin{equation}\label{entropy}
\frac1n \mathcal L(\theta) = \frac1n\sum_{i=1}^n\log(f(X_i| \theta)) \xlongrightarrow{n\to\infty} \mathbb E\big[\log(f(X| \theta))\big].
\end{equation}
Hence the maximiser of the left hand side should be close to the maximiser of the right hand. Differentiating the right hand side yields
\begin{equation*}
\begin{split}
\partial_\theta \mathbb E\big[\log(f(X| \theta))\big] & = \mathbb E\left[\partial_\theta \log(f(X| \theta))\right] = \mathbb E\left[\frac{\partial_\theta f(X| \theta)}{f(X| \theta)}\right] \\
& = \int \frac{\partial_\theta f(x| \theta)}{f(x| \theta)} f(x| \theta_0) \mu(\mathrm{d} x).
\end{split}
\end{equation*}
Evaluating this at \(\theta = \theta_0\) gives
\begin{equation*}
\int \partial_\theta f(x| \theta) \mu(\mathrm{d}x) = \partial_\theta \int f(x| \theta) \mu(\mathrm{d}x) = \partial_\theta(1) = 0.
\end{equation*}
Hence \(\theta_0\) is a critical point and under mild conditions the right hand side of \eqref{entropy} is concave and thus \(\theta_0\) is the unique maximiser. In conclusion the estimator \(\hat \theta\) should be close to \(\theta_0\).
\end{emp}

Although the rough structure of the rigorous proof is present in the argument above it is highly formal. For example we argue that if a sequence \((f_n)_{n\in\mathbb N}\) of functions converges towards \(f\) pointwise, then the maximisers \((x_n)_{n\in\mathbb N}\) should converge to the maximiser \(x\) of \(f\). The major tool to make this rigorous will be to use some kind of uniform convergence. Namely, we have the following result where we will omit the proof since it is very easy and we give a similar but stronger version of it later.

\begin{lem}[Swapping limit and maximisation]
Let \((f_n)_{n\in\mathbb N}\) be a sequence of real functions on a compact space with maximisers \((x_n)_{n\in\mathbb N}\) that are bounded from above and converge uniformly towards \(f\). Further, assume that \(f\) is continuous and has a unique maximum in \(x_0\). Then we have \(x_n\to x_0\) for \(n\to\infty\).
\end{lem}

Unfortunately the convergence in \eqref{entropy} does only hold uniformly on a compact set \(K\subseteq\Theta\). To deal with this, we will argue that the maximisers \((x_n)\) lie in this compact set \(K\) for large \(n\). We will do this in a general setup in the next paragraph.

\subsubsection*{A general consistency result for extremal estimators}

We provide a general consistency result for a rather broad class of estimators which is taken from \cite{newey1994large} and slightly adapted to our needs. Although it would be possible to prove the consistency of the MLEs directly we present this general procedure since it clearly highlights the theoretical arguments and can therefore easily be adjusted to other cases.

\begin{emp}[Setting]
Let in the following \(\Theta\) be a topological Hausdorff space and 
\(F_n \colon \Theta\to [-\infty, \infty)\)
be a sequence of random functions with maximisers
\[\hat{\theta}_n \coloneqq \underset{\theta\in\Theta}{\arg\max} F_n(\theta).\]
If no maximiser exists, we choose \(\hat{\theta}_n\in\Theta\) arbitrary. Further, let \(F\colon \Theta\to [-\infty, \infty)\) be a deterministic function with maximiser \(\theta_0\). The maximisers \(\hat{\theta}_n\) are called \emph{extremal estimators} since they are extremal points of the functions \(F_n\).
\end{emp}

We now investigate whether the extremal estimators converge to the maximiser \(\theta_0\).

\begin{theo}[Consistency of extremal estimators]\label{conext}
Let the setting be as above and assume that the following conditions hold.
\begin{enumerate}
\item Assume that there is \(\varepsilon_0>0\) and a compact set \(K_0\) containing \(\theta_0\), such that with probability tending to one
\begin{equation}\label{con2}
F_n(\theta) \le F(\theta_0) - \varepsilon_0 \quad \text{for all } \theta\notin K_0.
\end{equation}
\item Let \(F_n\) converge to \(F\) uniformly on \(K_0\) in probability, i.e. for any \(\varepsilon>0\) we have with probability tending to one
\begin{equation}\label{con1}
\big\lvert F_n(\theta) - F(\theta) \big\rvert \le \varepsilon \quad\text{for all }\theta\in K_0.
\end{equation}
\item Let \(F\) have a unique maximum at \(\theta_0\in\Theta\).
\item Assume that \(F\) is upper semicontinuous in the sense that
\[\left\{ \theta\in\Theta \mid F(\theta)\ge\alpha\right\}\subseteq\Theta\]
is closed for all \(\alpha\in\mathbb R\).
\item With probability tending to one \(F_n\) admits a maximiser.
\end{enumerate}
Then we have \(\hat{\theta}_n\to\theta_0\) in probability, i.e.
\[\mathbb P\left( \hat{\theta}_n\in U\right) \xlongrightarrow{n\to\infty} 1 \]
for any open subset \(U\subseteq\Theta\) containing \(\theta_0\).
\end{theo}
\begin{proof}
Note that it suffices to show \(\hat{\theta}_n\in U\) whenever \eqref{con2} and \eqref{con1} hold and \(F_n\) admits a maximiser. From here on the proof is of purely analytic content.

Fix now an open set \(U\subseteq\Theta\) that contains \(\theta_0\). Choosing \(\varepsilon<\varepsilon_0\) in (\emph{ii}) and using (\emph{i}) yields
\[F_n(\theta_0) \ge F(\theta_0) - \varepsilon > F(\theta_0) - \varepsilon_0 \ge F_n(\theta) \quad \text{for all } \theta\notin K_0. \]
Hence the maximum of \(F_n\) is attained in \(K_0\) and we have \(\hat{\theta}_n\in K_0\). Thus if \(K_0\subseteq U\) we are done.
If this is not the case \(F\) attains its maximum \(\alpha\) on \(K_0\setminus U\) because \(F\) is upper semicontinuous and \(K_0\setminus U\) is compact. Further, (\emph{iii}) implies \(\alpha < F(\theta_0)\) and thus we have 
\[K_0\cap \Big\{ \theta\in\Theta\mid F(\theta) > \alpha \Big\} \subseteq U. \]
 
So in order to show \(\hat{\theta}_n\in U\), it suffices to show \(F(\hat{\theta}_n)>\alpha\). However, (\emph{ii}) implies
\[F(\hat{\theta}_n) \ge F_n(\hat{\theta}_n) - \varepsilon \ge F_n(\theta_0) - \varepsilon \ge F(\theta_0) - 2\varepsilon > \alpha\]
for \(\varepsilon\) small enough.
\end{proof}

\begin{rem}
The theorem above might seem artificially general at first, but one has a high interest in consistency results at least for extremal estimators in metric spaces. Those can be used to deduce the consistency for the estimation of a function in a function space rather than a finite dimensional parameter.
\end{rem}

If we want to apply the previous result to the case of maximum likelihood estimation we need to set
\[F_n(\theta) \coloneqq \frac1n\sum_{i=1}^n\log(f(X_i| \theta)).\]
Note that the factor \(\frac1n\) does not change the maximum. However, \eqref{entropy} already gives the almost surely pointwise limit of those functions and if condition (\emph{ii}) of the previous statement should hold, we have to define
\[F(\theta) \coloneqq \mathbb E\big[\log(f(X| \theta))\big]. \]
The quantity \(F\) is -- up to the sign -- known as the \emph{entropy} and plays an important role in many different fields, for example statistical mechanics, applied statistics and information theory. For further reading we refer to \cite{martin2011mathematical}, \cite{mackay2003information}, \cite{volkenstein2009entropy} and \cite{gray1990entropy}.


\subsubsection*{Information inequality and locally uniform convergence}

The second and third requirement of the previous result can be proved in a general setting and without quantitative assumption and we adapt an argument from \cite{newey1994large} to fit our needs. In order to do this we will work with the following assumptions.

\begin{emp}[Setting]
Let in the following \(\Theta\) be a set and let 
\[\mathcal F = \Big\{ f(\cdot| \theta)\colon\mathcal  X\to[0, \infty) \;\big\lvert\;\theta \in\Theta\Big\}\]
be a family of probability densities on some measurable space \(\mathcal X\) with respect to some measure \(\mu\). Further, fix \(\theta_0\in\Theta\) and let \(X\) be distributed according to \(f(\cdot| \theta_0) \mathrm{d}\mu\), hence we have
\[\mathbb E\big[h(X)\big]=\int h(x)f(x| \theta_0) \mu(\mathrm d x).\] 
Let \((X_n)_{n\in\mathbb N}\) be a sequence of independent random variables distributed according to \(f(\cdot| \theta_0) \mathrm{d}\mu\). Finally define
\[F_n(\theta) \coloneqq \frac1n\sum_{i=1}^n\log(f(X_i| \theta))
\quad\text{and } F(\theta)\coloneqq \mathbb E\big[\log(f(X| \theta))\big]. \]
\end{emp}

\begin{prop}[Information inequality]\label{infine}
Let the setting be as above and assume that the parameter \(\theta_0\in\Theta\) is \emph{identifiable}, i.e. we have \(f(\cdot| \theta)\ne f(\cdot| \theta_0)\) whenever \(\theta\ne\theta_0\). Let further
\[\sup_{x\in \mathcal X, \theta\in\Theta} f(x| \theta)<\infty \quad \text{and } F(\theta_0)>-\infty .\]
Then the entropy
\[F(\theta) = \mathbb E\big[\log(f(X| \theta))\big] \]
has a unique maximum in \(\theta_0\).
\end{prop}
\begin{proof}
Let \(\theta\ne\theta_0\), then we either have \(F(\theta) = -\infty < F(\theta_0)\) or
\begin{equation}\label{entropybound}
F(\theta) = \mathbb E\big[\log(f(X| \theta))\big] > - \infty.
\end{equation}
In this case we want to exploit the strict Jensen inequality (cf. \cite{lehmann2006theory}) that yields for any positive random variable \(Y\) with finite expectation that is not constant
\[\mathbb E\big[\log(Y)\big] < \log(\mathbb E[Y]).\]
We set \(Y\coloneqq \frac{f(X| \theta)}{f(X| \theta_0)}\). This is positive \(f(\cdot| \theta_0) \mathrm d\mu\) almost everywhere because otherwise \eqref{entropybound} could not hold. Since \(\theta_0\) is identifiable, the random variable \(Y\) is not constant and we will see in the following computation that the expectation is finite. Now we obtain
\begin{equation*}
\begin{split}
F(\theta) - F(\theta_0) & = \mathbb E\big[\log(f(X| \theta))\big] - \mathbb E\big[\log(f(X| \theta_0))\big] = \mathbb E\left[\log\left( \frac{f(X| \theta)}{f(X| \theta_0)}\right)\right] \\
& < \log\left( \mathbb E\left[\frac{f(X| \theta)}{f(X| \theta_0)}\right]\right) = \log\left( \int f(x| \theta) \mu(\mathrm d x) \right) = 0.
\end{split}
\end{equation*}
\end{proof}

Next we take care of the second requirement of the consistency result. Namely we will show that the functions \(F_n\) associated with the MLE almost surely converge to \(F\) locally uniformly under fairly mild conditions. For this we modify the proof of a more general convergence result in \cite{tauchen1985diagnostic}.

\begin{lem}[Locally uniform convergence]\label{locunicon}
Let the setting be as above, but let \(\Theta\) be a metric space and let \(K\subseteq\Theta\) be compact such that the following conditions hold.
\begin{enumerate}
\item Let
\[\mathbb E\left[\sup_{\theta\in K} \big\lvert \log(f(X| \theta)) \big\rvert \right]<\infty.\] 
\item For every \(\theta\in K\) we have \(\log(f(\cdot, \gamma))\to\log(f(\cdot| \theta))\) almost surely with respect to \(f(\cdot| \theta_0) \mathrm d\mu\) for \(\gamma\to\theta\).
\end{enumerate}
Then we almost surely have \(F_n\to F\) uniformly on \(K\), i.e. almost surely
\[\sup_{\theta\in K} \big\lvert F_n(\theta) - F(\theta) \big\rvert \xlongrightarrow{n\to\infty} 0. \]
\end{lem}
\begin{proof}
Fix \(\varepsilon>0\) and define for \(x\in\mathcal X\) and \(\rho>0\)
\[u(x, \theta, \rho)\coloneqq \sup_{d(\gamma, \theta)\le \rho} \big\lvert \log(f(x| \gamma)) - \log(f(x| \theta)) \big\rvert  \xlongrightarrow{\rho\to0} 0\]
almost surely for \(\theta\) fixed where we used condition (\emph{ii}).

This in combination with (\emph{i}) and the dominated convergence theorem implies that the convergence also holds in expectation and therefore we have
\[\mathbb E\big[u(X, \theta, \rho)\big] \le \varepsilon \quad \text{for } \rho\le \delta(\theta).\]
The open balls \(B_{\delta(\theta)}(\theta)\) with center \(\theta\) and radius \(\delta(\theta)\) cover the compact set \(K\) and hence we can select a finite subcover
\[K \subseteq\bigcup_{k=1}^m B_{\delta(\theta_k)}(\theta_k). \]
Further we set
\[\mu_k\coloneqq \mathbb E\big[u(X, \theta_k, \delta(\theta_k))\big] \le \varepsilon.\]
Let \(\theta\in K\) and choose \(k\) such that \(\theta\in B_{\delta(\theta_k)}(\theta_k)\), then we can conclude
\begin{equation*}
\begin{split}
\left\lvert F_n(\theta) - F(\theta) \right\rvert \le &\; \frac{1}{n} \sum_{i=1}^n \big\lvert \log(f(X_i| \theta)) - \log(f(X_i| \theta_k)) \big\rvert \\
& + \left\lvert \frac1n\sum_{i=1}^n \log(f(X_i| \theta_k)) - F(\theta_k) \right\rvert + \big\lvert F(\theta_k) - F(\theta) \big\rvert \\
\le &\; \left( \frac1n\sum_{i=1}^n u(X_i, \theta_k, \delta(\theta_k)) - \mu_k \right) + \mu_k + 2\varepsilon \\
\le &\; 4 \varepsilon
\end{split}
\end{equation*}
almost surely for \(n\ge N(\varepsilon)\) where we used the strong law of large numbers twice and
\[\big\lvert F(\theta_k) - F(\theta) \big\rvert \le \mathbb E\left[ u(X, \theta_k, \delta(\theta_k)) \right] \le \varepsilon. \]
\end{proof}

\subsubsection*{Consistency of the MLEs for the quality and elementary kernel}

In this part we will -- for the first time -- make use of the specific structure of the model. Since we have already taken care of the conditions (\emph{ii})-(\emph{v}) of the general consistency result, we dedicate ourselves to the first requirement of Theorem \ref{conext}. For this we keep the setting of the previous section although we now consider the case that
\[\mathcal F = \Big\{ f(\cdot| \theta)\colon 2^{\mathcal Y}\to[0, \infty) \mid\theta \in\Theta\Big\}\]
 is one of the parametric families for the \(L\)-ensembles introduced in \ref{models}. Further, we denote a realisation of a DPP by \(\mathbf Y\) like earlier.

\begin{lem}[Control outside of a compact set]\label{concom}
The requirement \emph{(}i\emph{)} from Theorem \ref{conext} is satisfied for the three kinds of parametric families for the kernel estimation. Further, the compact set \(K_0\) can be chosen as follows. Let \(\mathcal A\) be the family of subsets \(A\subseteq\mathcal Y\) with positive probability \(f(A| \theta_0)>0\) and choose \(c(A)>0\) such that
\[-c(A) < \frac{2\cdot F(\theta_0)}{f(A|\theta_0)}.\]
Then we set
\[ K_0 \coloneqq \Big\{ \theta\in\Theta \;\big\lvert\; \log(f(A| \theta)) \ge - c(A) \text{ for all } A\in\mathcal A \Big\}.\]
\end{lem}
\begin{proof}
At first we note that \(F(\theta_0)>-\infty\). Let now
\[\hat{\mathbb P}_n\coloneqq \frac1n \sum_{i=1}^n \delta_{\mathbf Y_i} \]
 be the empirical measure. We have by the law of large numbers
\[\mathbb P\left( \hat{\mathbb P}_n(A) \ge \frac{f(A| \theta_0)}{2}\right)\xlongrightarrow{n\to\infty} 1\]
and so we can assume \(\hat{\mathbb P}_n(A) \ge \frac{f(A| \theta_0)}{2}\), since we are only interested in proving a statement with probability tending to one. For \(A\in\mathcal A\) we note that 
\[K_A \coloneqq \Big\{ \theta\in\Theta \;\big\lvert\; \log(f(A| \theta)) \ge - c(A) \Big\}\]
is closed since \(f(A| \theta)\) is upper semicontinuous. Further, \(K_A\) is compact for \(A=\varnothing\in\mathcal A\) since \(\log(f(\varnothing| \theta))\) is coercive in \(\theta\) which has been shown in the coercivity proofs earlier in this chapter. Further, it contains \(\theta_0\) as
\[\log(f(A| \theta_0)) \ge 2 \cdot \log(f(A| \theta_0)) > -c(A) \]
because \(f(A| \theta)\le 1\) and
\[\frac{F(\theta_0)}{f(A|\theta_0)} = \frac{\sum_{B\subseteq\mathcal Y} f(B|\theta_0)\log(f(B|\theta_0)) }{f(A|\theta_0)} \le \log(f(A|\theta_0)). \]
Now
\[K_0= \bigcap_{A\in \mathcal A} K_A\]
 is compact because \(K_\varnothing\) is compact. Fix \(\theta\notin K_0\), lets say \(\theta\notin K_A\), then we get
\begin{equation*}
\begin{split}
F_n(\theta) & = \int \log(f(x| \theta)) \hat{\mathbb P}_n(\mathrm{d}x) = \sum_{B\in \mathcal A} \hat{\mathbb P}_n(B)\cdot \log(f(B| \theta)) \le \hat{\mathbb P}_n(A)\cdot \log(f(A| \theta)) \\
& < - \frac{f(A| \theta_0)}{2} \cdot c(A) < F(\theta_0).
\end{split}
\end{equation*}
\end{proof}

Now we have all the auxiliary results to prove the desired consistency result.

\begin{theo}[Consistency]
\begin{enumerate}
\item The maximum likelihood estimator \(\hat{L}_n\) for the elementary kernel is consistent. Namely if the observations \((\mathbf Y_n)\) follow the law of a \(L\)-ensemble with elementary kernel \(L_0\), then we have
\[\mathbb P\left( d(\hat L_n, L_0) \le \varepsilon\right) \xlongrightarrow{n\to\infty} 1 \quad \text{for all }\varepsilon>0.\]
\item The maximum likelihood estimator \(\hat{q}_n\) for the quality vector is consistent. Namely if the observations \((\mathbf Y_n)\) follow the law of a \(L\)-ensemble with kernel \(\Psi(q_0, \hat \phi)\), then we have
\[\mathbb P\Big( \big\lVert \hat q_n - q_0 \big\rVert \le \varepsilon\Big) \xlongrightarrow{n\to\infty} 1\quad \text{for all }\varepsilon>0.\]
\item Suppose that the observations \((\mathbf Y_n)\) follow the law of a \(L\)-ensemble with kernel \(\Psi(p_0, \hat S)\) where \((p_0)_i = \exp(\theta_0^Tf_i)\). Then we have
\[\mathbb P\left( \big\lVert\hat \theta_n - \theta_0 \big\rVert \le \varepsilon\right) \xlongrightarrow{n\to\infty} 1\quad \text{for all }\varepsilon>0.\]
\end{enumerate}
\end{theo}
\begin{proof}
We will only sketch the main parts of the proof of the second statement, since all other arguments will be analogue and therefore redundant.

Obviously we want to exploit the machinery we have introduced and thus we will check the requirements of Theorem \ref{conext}. First we note that (\emph{v}) holds because of the section of the existence of the maximum likelihood estimators.

We can express the entropy function
\begin{equation}\label{ent}
F(q) = \mathbb E\big[\log(f(\mathbf Y| q))\big] = \sum_{A\subseteq\mathcal Y} \log(f(A| q)) f(A| q_0)
\end{equation}
where the elementary probabilities are given by
\begin{equation}\label{elepro}
f(A| q) = \frac{\prod_{i\in A} q_i^2 \det(\hat S_A) }{\sum_{B\subseteq\mathcal Y} \prod_{i\in B} q_i^2 \det(\hat S_B) }
\end{equation}
which is continuous in \(q\). Hence, the entropy function \(F\) is upper semicontinuous and thus condition (\emph{iv}) holds.

To check that (\emph{iii}) holds, we will use the information inequality \ref{infine}. First we note that because of
\[f(\left\{ i\right\}| q) \propto q_i^2 \]
the parameter \(q_0\) is identifiable and further we have
\[\sup_{A\subseteq\mathcal Y, q\in\mathbb R_+^N} f(A| q) \le 1 \]
since the densities are elementary probabilities. Finally \(F(q_0)>-\infty\) is clear from \eqref{ent} and hence the third requirement is satisfied.

Since the previous lemma already takes care of condition (\emph{i}) it suffices to show the second condition for which we will use \ref{locunicon}. Hence, it remains to check the two conditions of this lemma, but the second one -- the continuity condition -- obviously holds as can be seen from \eqref{elepro}. To see that the first one also holds, we note that for \(A\subseteq\mathcal Y\) with \(f(A| q_0)>0\) and \(q\in K_0\) we have
\[0 \ge \log(f(A| q)) \ge -c(A) > -\infty.\]
Hence the random variable
\[\sup_{q\in K_0} \big\lvert \log(f(\mathbf Y| q)) \big\rvert\]
is almost surely finite with respect to \(f(\cdot|q_0)\mathrm{d}\mu\) and since the probability space \(2^{\mathcal Y}\) is finite, the second condition holds.
\end{proof}

\begin{rem}
Obviously in the proof of the consistency of the whole elementary kernel \(L\) one runs into the problem of unidentifiability. This is why one has to identify the parameters with each other that give rise to the same elementary probabilities which is just the determinantal equivalence. Once this is done, the proof follows analogously.
\end{rem}

\subsubsection*{Consistency of regularised MLE}

One can not make a general statement about the consistency of regularised MLE. Since it is straight forward to construct examples where the regularised MLE is not consistent. For example if the regulariser \(R\) is equal to \(-\infty\) on a neighborhood of \(\theta_0\), then none of the regularised estimators will lie in this neighborhood and hence the estimations will not converge towards \(\theta_0\).

Nevertheless, this scenario can be avoided with one property of the regulariser. If we recall that the proof of the consistency of the MLE relied on the observation
\[\frac1n\cdot\mathcal L_n(\theta)\xlongrightarrow{n\to\infty} \mathbb E\big[\log(f(X|\theta))\big]. \]
and an application of the consistency result for extremal estimators. In the case of the regularised MLE, we no longer maximise the functions \(\mathcal L_n\) but the sum \(\mathcal L_n + R\) and in order to obtain a consistent estimator we need to ensure that the convergence
\[\frac1n\cdot\mathcal L_n(\theta) + \frac1n \cdot R(\theta) \xlongrightarrow{n\to\infty} \mathbb E\big[\log(f(X|\theta))\big] \]
holds with the same uniformity which we will do now.

\begin{emp}[Setting]
We work with one of the three parametric models for DPPs presented above and denote the parameter space by \(\Theta\) and the elementary kernel arising from \(\theta\in\Theta\) by \(L(\theta)\). Further we will assume that we have a regulariser \(R\colon\Theta\to[-\infty, 0]\) that is upper semicontinuous.
\end{emp}

\begin{theo}[Consistency of MAP estimation]
Assume that the sequence of observations \((\mathbf Y_n)_{n\in\mathbb N}\) are independent and distributed according to a \(L\)-ensemble with elementary kernel \(L(\theta_0)\). Let further \(K\subseteq\Theta\) be a compact set containing \(\theta_0\) such that \(R\) is bounded on \(K\). Then the maximum a posteriori estimator arising from the regulariser \(R\) is consistent.
\end{theo}
\begin{proof}
We only have to check how the regulariser \(R\) influences the requirements (\emph{i})-(\emph{v}) of Theorem \ref{conext}. In fact it doesn’t have any influence on the assumptions (\emph{iii}) and (\emph{iv}). Further (\emph{i}) stays valid since the regulariser is non positive and (\emph{v}) since it is non positive and upper semicontinuous. Hence it remains to validate (\emph{ii}). For this we reduce the compact set \(K_0\) to \(K\cap K_0\) which is compact again. On this set the contribution \(\frac1n \cdot R\) goes uniformly to zero which yields the assertion.
\end{proof}

\begin{rem}
The consistency result above is stated for a rather broad class of regularisers. For instance, all quadratic regulariser -- which correspond exactly to Gaussian priors as we will see later -- fall into this class. Further it covers every regulariser that is equal to \(-\infty\) on a set of impossible parameters. This can be used to exclude non symmetric or non positive semidefinite matrices from the estimation process if one wants to work with the easier parameter set \(\mathbb R^{N\times N}\) instead of \(\mathbb R^{N\times N}_{\text{sym}, +}\).
\end{rem}

\subsection{Approximation of the MLE}

Having discussed the theoretical properties and guarantees on convergence of the MLE we will now turn towards the question of computability. In particular we will see that the MLE for the whole kernel can not be computed in an efficient way which justifies the use of smaller parametric models like the log linear model.

\subsubsection*{Likelihood maximisation for the elementary kernel \(L\)}

We recall that the log likelihood function for the elementary kernel is given by
\begin{equation}\label{loglikagain}
\mathcal L(L) = \sum\limits_{i=1}^n\log\left( \det(L_{\mathbf Y_i})\right) - n \log\left( \det(L+I)\right).
\end{equation}
This is a smooth function since the determinants of the submatrices are polynomials in the entries of \(L\) and the composition of those with the smooth function \(\log\colon(0,\infty)\to\mathbb R\) is smooth. This property makes it possible to use gradient methods for the maximisation of \(\mathcal L\), but they face the problem that \(\mathcal L\) is non concave and thus those algorithms will generally not converge to a global maximiser. To see that the log linear likelihood function is not concave, we may consider the span \(\left\{ q I\mid q\in\mathbb R\right\}\) of the identity matrix. On this subspace \(\mathcal L\) takes the form
\[\mathcal L(q I) = \sum\limits_{i = 1}^n \log(q^{\left\lvert Y_i \right\rvert}) - n \log((1 + q)^N) = \sum\limits_{i = 1}^n \left\lvert Y_i \right\rvert \log(q) - n N \log(1 + q) \]
which is not concave in general. Unfortunately there are no algorithms that can approximate the global maximum of a non concave function \cite{vavasis1995complexity} and it also has been conjectured in \cite{kulesza2012learning} that no such algorithm exists for the log likelihood of the elementary kernel. Nevertheless one can still use optimisation techniques to obtain local maximisers of the log likelihood and indeed \cite{mariet2015fixed} proposes a fixed point iteration to do this.

In fact, the non concavity can be seen without any computations at all. If we have a maximiser \(L\) of the log likelihood function and take another matrix \(\tilde L\) that is determinantally equivalent, then obviously the log likelihood of the two kernels agree. Hence, we expect a discrete set of maximisers which is only possible if \(\mathcal L\) is not concave. However it is not straight forward whether there are some critical points that are different to the global maximisers. If this is not the case, common optimisation techniques can be exploit since they will converge to a critical point.

Similar arguments show that the log likelihood function for the qualities is non concave and therefore hard to maximise.

\subsubsection*{Computation for the log linear model}

The motivation to introduce the log linear model was to obtain a log likelihood function that is easier to maximise in practice. We will now see that this is indeed the case and recall that the individual terms of the log likelihood are given by
\begin{equation}\label{logliklogmodagain}
2\cdot\theta^T\sum\limits_{i\in Y}f_{i} + \det(\hat{S}_{Y}) - \log\left( \sum_{A\subseteq\mathcal Y} \exp\left(2\cdot \theta^T\sum_{i\in A}f_{i}\right)\det(\hat{S}_A) \right).
\end{equation}

The first two terms are linear in \(\theta\) and constant and thus concave. To see that the last expression is also concave we introduce the notion of log concavity and give a general result.

\begin{defi}[Log concavity]
We call a function \(f\) \emph{log concave}, \emph{log convex} or \emph{log (affine) linear} if \(\log(f)\) has the respective property.
\end{defi}

\enlargethispage{.3cm}
\begin{prop}[Additivity of log concavity]
The sum of log convex functions is again log convex.
\end{prop}
\begin{proof}
Let \(f\) and \(g\) be log convex and thus \(F\coloneqq\log(f)\) and \(G\coloneqq\log(g)\) are convex. We will consider the function
\[H\colon\mathbb R^2\to\mathbb R, \quad (x, y)\mapsto \log(e^x + e^y)\]
which is increasing both coordinates. Further, we note that \(\log(f + g) = H(F, G)\) and hence it suffices to show that \(H\) is convex which we do by noting that the Hessian matrix
\[D^2H(x, y) = (e^x + e^y)^2 \cdot\begin{pmatrix}
e^xe^y & -e^xe^y \\ -e^xe^y & e^xe^y
\end{pmatrix}\]
is positive semi-definite by Sylvester’s criterion.
\end{proof}

The summands inside the logarithm in \eqref{logliklogmodagain} are log convex, in fact even log affine linear since their logarithm is equal to
\[2\cdot\theta^T\sum\limits_{i\in A}f_{i} + \log(\det(\hat{S}_{A})).\]
Hence we obtain as an immediate consequence that the whole expression \eqref{logliklogmodagain} and therefore the log likelihood function is concave which we will fix in a separate statement.

\begin{cor}[Concavity of the likelihood function]
Under the log linear model for the qualities, the log likelihood function is concave in the log linearity parameter \(\theta\in\mathbb R^M\).
\end{cor}

This result together with the coercivity -- which holds with probability tending to one -- ensures that the MLE for the log linearity constant of the quality can be computed efficiently. Further, such optimisation algorithms are pre-implemented in most major programming languages and software environments like Mathematica, MATLAB and R. For theoretical guarantees and algorithmic details of those methods we refer once again to \cite{boyd2004convex}.

Although being easily available, those methods have the drawback that they need to approximate the gradient of the log likelihood function. However, the gradient can be expressed analytically by differentiating \eqref{logliklogmodagain} and we obtain
\begin{equation*}
\begin{split}
& 2\cdot\sum\limits_{i\in Y}f_{i} - 2\cdot \frac{ \sum_{A\subseteq\mathcal Y} \exp\left(2\cdot \theta^T\sum_{i\in A}f_{i}\right)  \sum_{i\in A}f_{i} \det(\hat{S}_A)}{ \sum_{A\subseteq\mathcal Y} \exp\left(2\cdot \theta^T\sum_{i\in A}f_{i}\right)\det(\hat{S}_A)} \\
= & \; 2\cdot\sum\limits_{i\in Y}f_{i} - 2\cdot\sum_{A\subseteq\mathcal Y} f(A|\theta) \sum_{i\in A}f_{i} \\
= & \; 2\cdot\sum\limits_{i\in Y}f_{i} - 2\cdot\sum_{i\in \mathcal Y} f_i \sum_{A\supseteq \left\{ i\right\}}f(A|\theta) \\
= & \; 2\cdot\sum\limits_{i\in Y}f_{i} - 2\cdot\sum_{i\in \mathcal Y} f_i K(\theta)_{ii}.
\end{split}
\end{equation*}
where \(K(\theta)\) is the marginal kernel arising from the parameter \(\theta\). This expression can be used in a direct implementation of the gradient method for the approximation of the MLE for the log linearity constant. 

\subsection{Further learning approaches}

We will quickly touch on two approaches of parameter estimation that have proven to work very well for certain real world examples. Further, we propose a different parametric model that would allow the simultaneous estimation of the qualities and the similarity kernel, but it remains to be seen whether this circumvents the problems associated with the maximum likelihood estimation of the whole kernel \(L\).

\subsubsection{Learning for conditional DPPs}

The estimation of the log linearity constant of conditional DPPs has been used in \cite{kulesza2012learning1} to obtain extractive summaries of news articles. In fact the procedure is analogue to the one presented for the estimation of the log linearity constant of normal DPPs, apart from the fact that one has to model a family of feature vectors
\[f_i(X) \in\mathbb R^M \quad \text{for } i\in \mathcal Y(X), X\in\mathcal X. \]

We will quickly discuss how this could be done in the case of the DPP on a two dimensional grid. Maybe we do not want to restrict ourselves to one grid size and hence consider the conditional DPP
\[\mathcal X = \mathbb N, \quad \mathcal Y(n) = n^{-1}\left\{ 0, \dots, n\right\}^2 \quad \text{for } n\in\mathbb N.\]
Now we can model the similarity feature vectors \(\phi_i(n)\) analogously to the case where we only considered one grid and impose the following log linear model for the qualities
\[q_i(n) = \exp\big(\theta^Tf_i(n)\big) \quad \text{for } i\in\mathcal Y(n). \]
The diversity feaure vectors are given just like earlier by
\[f_i(n) = \begin{pmatrix}
\left\lVert i - m \right\rVert \\ 1
\end{pmatrix} \quad\text{for } i\in\mathcal Y(n) \]
where \(m\) is again the centre of the unit square. Now the estimation can be carried out just like in the case of an ordinary DPP and for the same reason this will be consistent.

However, it shall be noted that the modelling of the diversity feature vectors can be far more complicated in more complex real world applications.

\subsubsection{Estimating the mixture coefficients of \(k\)-DPPs}

For this approach we first fix symmetric positive semi-definite matrices \(L_1, \dots, L_M\). We assume now that the point process we are trying to describe is the mixture of the DPPs \(\mathbb P_{L_m}\) with elementary kernel \(L_m\) and aim to estimate the mixing coefficients of
\[\mathbb P_\theta = \sum_{m=1}^M \theta_m\mathbb P_{L_m} \quad \text{where } \theta_m\in[0, 1] \text{ and } \sum_{m=1}^M\theta_m=1.\]

This approach has been taken to create a diverse selection of pictures returned by an image search (cf. \cite{kulesza2011k}). In this case one wants to fix the number of returned images up front, hence it is reasonable to work with \(k\)-DPPs instead of DPPs. The mixture coefficients were estimated based on a data set \(\big\{ (Y_t^+, Y_t^-)\big\}_{t = 1, \dots, n}\) where \(Y_t^+\) was chosen by a human to be more diverse than \(Y_t^-\). Now \(\theta\) was optimised such that 
\[\mathbb P_\theta(Y_t^+) > \mathbb P_\theta(Y_t^-) \]
for as many \(t\in\left\{ 1, \dots, n\right\}\) as possible.\footnote{For people familiar with binary decision problems it should be mentioned that this was done using the logistic loss.}


\subsubsection{Learning the repulsiveness of a DPP}

The estimation of the qualities, or the according log linearity constant have the major drawback that a significant part of the DPP -- the repulsive structure -- has to be modelled completely. We have seen so far that it is in practice not possible to estimate the whole repellent structure, namely the similarity kernel as this would lead to a hard optimisation problem. However, we will propose a parametrisation of the similarity kernel by only one parameter, that might have better computational properties.

For this we follow \ref{moddiv} to model the similarity over the distance to some reference points with respect to a Gaussian kernel just like in the toy example presented so far. This means we choose
\[(\phi_i)_r \propto \exp\left(- \frac{d(i, r)^2}{\sigma}\right) \quad\text{for } r\in\mathcal R, i\in\mathcal Y.\]
We have seen in the example of the DPP on a line that the parameter \(\sigma\) has a direct influence on the strength of the repulsions of the DPP. The estimation of not only the log linearity constant \(\theta\) but also the \emph{repulsiveness parameter} \(\sigma\) could now result in a significantly increased accuracy of the resulting model. 

It is not immediately clear what properties the log likelihood has in this case, but it would be -- a rather pleasant -- surprise if it was coercive. Nevertheless, it might have nice properties, like a unique critical point that could allow the use of standard optimisation techniques. A different approach would be to optimise the two parameters \(\theta\) and \(\sigma\) in an adaptive scheme, i.e. one after another and repeat this iteratively. It remains to be seen whether such approaches work theoretically and whether they give any improved results in practice.
