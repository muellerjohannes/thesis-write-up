\edef\myindent{\the\parindent}
\begin{center}
{\LARGE\textsc{Acknowledgements}\\[.9cm]}
% \normalsize
\begin{minipage}{11cm}\setlength{\parindent}{\myindent}
%First and foremost I would like to thank my supervisors Nikos and Theo for their guidance and advice along the path of this dissertation. It was a pleasure to work with you in such a friendly and inspiring atmosphere.

%I would like to thank the Evangelisches Studienwerk Villigst e.V. that gave me the opportunity to complete my MSc here at the University of Warwick through their financial and also their non-material support. Further I would like to thank Patrick Dondl who gave me good advice for the choice of the programme as well as some insights into the English university system.

%I have to thank my favourite radio station egoFM for the creation of their electronical channel FLASH which kept me well entertained during the long days of write up.

%I can’t thank my family enough for supporting me in every possible way during the last year. A special Dankeschön goes to my grandma for her sweet and original letters. 

%Further I want to thank those who did their best to distract me from my work and my dissertation, a special mention deserve all my visitors, the Pappenheim family and my mountaineering partners. Also, I want to thank my friends back home for always having a spot on their couches and an open ear for everything. A big thanks also to all the people I met during this year and who turned it into an amazing experience.
% and all the effort they put into convincing me to come back.

%I also want to thank \begin{otherlanguage}{greek} Gwg\'{w} \end{otherlanguage} for making the last stage of my thesis a lot more event- and joyful and for her crash course on Greek pronunciation.

%Last but by no means least, I would like to thank the dreamteam for being such an eggcelent friendgroup and for all the pun we had during the last year!
\end{minipage}
\end{center}

\clearpage


\begin{center}
{\LARGE\textsc{Abstract}\\[.9cm]}
\begin{minipage}{11cm}
%Determinantal point processes are random subsets that exhibit a diversifying behaviour in the sense that the randomly selected points tend to be not similar in some way. This repellent structure first arrose in theortical physics and pure mathematics, but they have recently been used to model a variety of many real world scenarios in a machine learning setup. We aim to give an overview over the main ideas of this approach which is easily accessible even without prior knowledge in the area of machine learning and sometimes omit technical calculations in order to keep the focus on the concepts.
Determinantal point processes (DPPs) are a probabilistic model of diverse subsets that exhibits desirable computational properties in terms of its simulation, marginalisation and other operations. That is why they have recently been used in an increasing amount of real world applications like text summarisation, the selection of a diverse subset of pictures returned by an image search or the selection of human poses in a picture (cf. \cite{kulesza2012learning}). A crucial step in all of those applications is the estimation of different parameters and this will be the focus of this dissertation. We will give an overview over two different types of point estimators and their benefits and hindrances and provide proofs for their consistency. The first one will be based on the reconstruction of a symmetric matrix from its principal minors and will allow to obtain an estimate for the marginal kernel based on the empirical measures. The second kind of estimation will be maximum likelihood estimation (MLE). We provide proofs for the consistency of those estimators including their regularised versions which can not be found in the literature so far. 
However, we will see that in practice one has to maximise the log likelihood function which is not concave in the case of DPPs and therefore not possible in an efficient way. This motivates the Bayesian approach to the estimation of those parameters and we will see how the posterior density can be approximated using different Markov chain Monte Carlo (MCMC) methods. Further we will provide toy examples for some of the presented estimation procedures. We will use those to investigate how the prior, or equivalently the regularisation of the MLE influence the estimation. In fact, we will see that they are not benefitial, not even under the presence of noise unless one knows exactly how the noise affects the estimation.  % can be used to lower the effect random perturbations of the observations have on the estimation but will see that this requires a certain degree of care.
\end{minipage}
\end{center}

%\todo{cite, or give more obvious examples}{ }

\clearpage
