\chapter{Summary and conclusion}

%\todo{General properties first?}
In this thesis we gave a short but mostly self contained introduction to the basic notions of discrete determinantal point processes. This also included results concerning their existence as well as an explicit construction of them which can be used to simulate them.

Based on those preliminaries we have presented different approaches to the estimation of several parametric models of discrete DPPs. First we presented a point estimator that reconstructs an estimation for the marginal kernel according to the empirical marginal densities. The central tool for this is the solution of the principle minor assignment problem that is concerned with the existence and construction of a matrix with prediscribed principle minors. We showed how this can be reduced to the solution of a set of linear equations over the prime field \(\mathbb F_2\). One drawback of this approach is, that one has to calculate a minimal shortest cycle basis of the estimated adjacency graph which is not straight forward to implement. Further we have seen that this estimator is consistent, but the results in \cite{urschel2017learning} also imply that the convergence might be very slow. % which again relies on the 

The second approach was based on the well established theory of maximum likelihood estimation, which yields another point estimator. However the main difficulty here is that the log likelihood function for the whole elementary kernel \(L\) is not concave and therefore hard to maximise in practice. Nevertheless, we have seen that this problem can be solved by the use of a log linear model for the qualities. The trade off is that this model has a lower descriptive power and that one has to model the similarity of the DPP which determines the structure and strength of the repulsion. Finally, we proved that the MLEs exist with increasing probability and that they are consistent estimators for the respective parameters which we also generalised to the MAP estimation. %\todo{add consistency}

In the third chapter we introduced the Bayesian approach to parameter estimation which is fundamentally different in the sense that it treats the estimated parameter as a random varibale rather than a single value. This does not only allow to capture the uncertainty of the estimation but also has a regularising effect in the sense that the posterior distribution always exists even if the according maximum likelihood estimator doesn’t. We have seen that the prior corresponds exactly to the explicit regularisation of the MLE we considered earlier. We have seen how the posterior density of the parameter can be approximated using different MCMC methods even if the MLE is impossible to compute. 

In the last chapter we performed the MLE and Bayesian estimation for a toy model introduced earlier. We saw how the sample size affects the quality of the estimation, in particular with different regularisations. In general, it can be noted that a wrong regularisation is especially bad for small sample sizes and hence different priors should be compared, for example using the Bayes factor. Further we used the MH random walk to approximate the posterior distribution and applied two different burn in periods. It is apparent that the tuning of the proposal lead to a higher acceptance rate and a faster decreasing auto correlation function. Finally we investigated the effect random noise has on the estimation. In fact, some parameters get distorted by the presence of Poisson noise, however we argue that it is only reasonable to use a suitable regulariser if one has a clear understanding of the qualitative structure of this influence.
%\todo{comment on examples and negative effect of regularisation}

%\begin{enumerate}
%\item We’ve seen different approaches to the parameter estimation of DPPs
%\item The first two we’re point estimators and we’ve seen that they are both consistent. However, the maximum likelihood estimation runs into computational problems since one has to maximise a non concave function.
%\item This and also the goal to capture uncertainty and to regularise the parameter estimation -- which will solve the problem that the MLE doesn’t always exist -- lead to the Bayesian approach to parameter estimation. Here the problem is that the normalisation constant of the posterior density can not be computed analytically or numerically. Nevertheless, the posterior density can be approximated through MCMC methods.
%\end{enumerate}•

\subsubsection{Further work}

During the work on this dissertation the following questions arose that might be worth to consider further. 

\begin{enumerate}
\item Can one effectively perform maximum likelihood estimation of the repulsiveness parameter \(\sigma\), in the best case even simultaneously to the log linearity constant \(\theta\) of the quality? If not, could this be done by iteratively optimising \(\sigma\) and \(\theta\) after another? If one of those procedures works theoretically, does it provide any significant improvement over the other estimations?
\item What does the geometry of the log likelihood function of the whole elementary kernel look like? Are there other critical points than to the global maximiser? %as a function of the determinantal equivalence classes? Is there a unique maximiser of the log likelihood function in this case and maybe even a unique critical point? %What is the structure of the set of those equivalence clases and can one effectively optimise functions on this set? %Then standard optimisation tools could be used. However, the structure of those equivalent classes is not obvious....
\item Are the presented point estimators unbiased? 
\item How do the different point estimators perform compared to each other and can one put those findings onto rigorous base in the sense that one is the optimal estimation for some given observations? Does that performance change under the presence of noise?
\item Investigate whether the ‚naive‘ approach for the approximation of the posterior could somehow be saved in higher dimension. Past work on the approximation of high dimensional functions could help here as well.
%say something about the grid method.
%Does the regularising effect of the prior bring any substantial benefits if one perturbs the observation with random noise? %How strong and beneficial is the regularising effect of the prior in terms of noise on the observation?
\item Find further applications for the use of DPPs.
\end{enumerate}

To conlude, we want to emphasise that we believe that determinantal point processes will continue to get attention from the research communities concerned with machine learning, data science and computational statistics. We assume that they can help to improve various current techniques in those fields and think that they are already on the way of doing this.  %-- although the focus of this dissertation was the estimation of the parameters of DPPs -- 
